{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing how coherence scales with model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_index</th>\n",
       "      <th>class_type</th>\n",
       "      <th>class</th>\n",
       "      <th>class_elicitation</th>\n",
       "      <th>evidence_category</th>\n",
       "      <th>evidence_text</th>\n",
       "      <th>evidence_elicitation</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>...</th>\n",
       "      <th>posterior_prob</th>\n",
       "      <th>prior_prompt</th>\n",
       "      <th>likelihood_prompt</th>\n",
       "      <th>posterior_prompt</th>\n",
       "      <th>prior_full_text</th>\n",
       "      <th>likelihood_full_text</th>\n",
       "      <th>posterior_full_text</th>\n",
       "      <th>prior_token_logprobs</th>\n",
       "      <th>likelihood_token_logprobs</th>\n",
       "      <th>posterior_token_logprobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>literary_analysis</td>\n",
       "      <td>works that bring out the contemporary social ...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-5.289000511169434, -2.5782995223999023, -6.3...</td>\n",
       "      <td>[-3.8323044776916504, -0.995391845703125, -1.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>literary_analysis</td>\n",
       "      <td>character-driven narratives.</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-10.583301544189453, -1.8038426637649536, -3....</td>\n",
       "      <td>[-4.018360137939453, -1.8006072044372559, -0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>historical_context</td>\n",
       "      <td>literature from periods of significant social...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-4.2250213623046875, -2.044250965118408, -7.6...</td>\n",
       "      <td>[-4.17518424987793, -1.5934702157974243, -1.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>historical_context</td>\n",
       "      <td>social observers.</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-10.307802200317383, -12.089680671691895, -2....</td>\n",
       "      <td>[-4.350062847137451, -2.5824971199035645, -0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>cultural_impact</td>\n",
       "      <td>books that challenged conventional thinking a...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-3.743206024169922, -2.3469152450561523, -11....</td>\n",
       "      <td>[-4.11558723449707, -1.5119941234588623, -0.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>financial_insight</td>\n",
       "      <td>innovative financing solutions enable more fl...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-11.379182815551758, -12.664813995361328, -6....</td>\n",
       "      <td>[-6.796856880187988, -2.6333279609680176, -2.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>travel_experience</td>\n",
       "      <td>immersive cultural interactions create the mo...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-9.444840431213379, -0.29459497332572937, -8....</td>\n",
       "      <td>[-7.761892318725586, -2.237544536590576, -1.62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>travel_experience</td>\n",
       "      <td>discovering off-the-beaten-path destinations ...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-12.130977630615234, -2.654900312423706, -0.3...</td>\n",
       "      <td>[-7.654887676239014, -2.525331735610962, -2.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>fitness_regimen</td>\n",
       "      <td>active travel, such as hiking and cycling, is...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-13.474319458007812, -2.316763401031494, -1.7...</td>\n",
       "      <td>[-7.427134990692139, -2.856992721557617, -1.80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>fitness_regimen</td>\n",
       "      <td>integrating fitness routines into travel enha...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-11.553764343261719, -8.011931419372559, -7.4...</td>\n",
       "      <td>[-7.912234306335449, -2.7759504318237305, -2.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5796 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_index          class_type                  class  \\\n",
       "0              0           novelists   William Shakespeare.   \n",
       "1              0           novelists   William Shakespeare.   \n",
       "2              0           novelists   William Shakespeare.   \n",
       "3              0           novelists   William Shakespeare.   \n",
       "4              0           novelists   William Shakespeare.   \n",
       "...          ...                 ...                    ...   \n",
       "4027           7  travel_preferences         luxury travel.   \n",
       "4028           7  travel_preferences         luxury travel.   \n",
       "4029           7  travel_preferences         luxury travel.   \n",
       "4030           7  travel_preferences         luxury travel.   \n",
       "4031           7  travel_preferences         luxury travel.   \n",
       "\n",
       "                          class_elicitation   evidence_category  \\\n",
       "0                    My favourite author is   literary_analysis   \n",
       "1                    My favourite author is   literary_analysis   \n",
       "2                    My favourite author is  historical_context   \n",
       "3                    My favourite author is  historical_context   \n",
       "4                    My favourite author is     cultural_impact   \n",
       "...                                     ...                 ...   \n",
       "4027   My travel style is best described as   financial_insight   \n",
       "4028   My travel style is best described as   travel_experience   \n",
       "4029   My travel style is best described as   travel_experience   \n",
       "4030   My travel style is best described as     fitness_regimen   \n",
       "4031   My travel style is best described as     fitness_regimen   \n",
       "\n",
       "                                          evidence_text  \\\n",
       "0      works that bring out the contemporary social ...   \n",
       "1                          character-driven narratives.   \n",
       "2      literature from periods of significant social...   \n",
       "3                                     social observers.   \n",
       "4      books that challenged conventional thinking a...   \n",
       "...                                                 ...   \n",
       "4027   innovative financing solutions enable more fl...   \n",
       "4028   immersive cultural interactions create the mo...   \n",
       "4029   discovering off-the-beaten-path destinations ...   \n",
       "4030   active travel, such as hiking and cycling, is...   \n",
       "4031   integrating fitness routines into travel enha...   \n",
       "\n",
       "           evidence_elicitation  \\\n",
       "0              I prefer reading   \n",
       "1              I prefer reading   \n",
       "2              I prefer reading   \n",
       "3              I prefer reading   \n",
       "4              I prefer reading   \n",
       "...                         ...   \n",
       "4027   I find that traveling by   \n",
       "4028   I find that traveling by   \n",
       "4029   I find that traveling by   \n",
       "4030   I find that traveling by   \n",
       "4031   I find that traveling by   \n",
       "\n",
       "                                   conversation_history  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                   model_name model_provider  ...  posterior_prob  \\\n",
       "0     meta-llama/Llama-3.1-8B             hf  ...        0.002039   \n",
       "1     meta-llama/Llama-3.1-8B             hf  ...        0.001275   \n",
       "2     meta-llama/Llama-3.1-8B             hf  ...        0.000455   \n",
       "3     meta-llama/Llama-3.1-8B             hf  ...        0.000438   \n",
       "4     meta-llama/Llama-3.1-8B             hf  ...        0.001370   \n",
       "...                       ...            ...  ...             ...   \n",
       "4027    EleutherAI/pythia-12b             hf  ...        0.000004   \n",
       "4028    EleutherAI/pythia-12b             hf  ...        0.000009   \n",
       "4029    EleutherAI/pythia-12b             hf  ...        0.000004   \n",
       "4030    EleutherAI/pythia-12b             hf  ...        0.000006   \n",
       "4031    EleutherAI/pythia-12b             hf  ...        0.000003   \n",
       "\n",
       "                                           prior_prompt  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                      likelihood_prompt  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                       posterior_prompt  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                        prior_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                   likelihood_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                    posterior_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                   prior_token_logprobs  \\\n",
       "0     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "1     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "2     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "3     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "4     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "...                                                 ...   \n",
       "4027  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4028  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4029  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4030  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4031  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "\n",
       "                              likelihood_token_logprobs  \\\n",
       "0     [-5.289000511169434, -2.5782995223999023, -6.3...   \n",
       "1     [-10.583301544189453, -1.8038426637649536, -3....   \n",
       "2     [-4.2250213623046875, -2.044250965118408, -7.6...   \n",
       "3     [-10.307802200317383, -12.089680671691895, -2....   \n",
       "4     [-3.743206024169922, -2.3469152450561523, -11....   \n",
       "...                                                 ...   \n",
       "4027  [-11.379182815551758, -12.664813995361328, -6....   \n",
       "4028  [-9.444840431213379, -0.29459497332572937, -8....   \n",
       "4029  [-12.130977630615234, -2.654900312423706, -0.3...   \n",
       "4030  [-13.474319458007812, -2.316763401031494, -1.7...   \n",
       "4031  [-11.553764343261719, -8.011931419372559, -7.4...   \n",
       "\n",
       "                               posterior_token_logprobs  \n",
       "0     [-3.8323044776916504, -0.995391845703125, -1.3...  \n",
       "1     [-4.018360137939453, -1.8006072044372559, -0.8...  \n",
       "2     [-4.17518424987793, -1.5934702157974243, -1.92...  \n",
       "3     [-4.350062847137451, -2.5824971199035645, -0.8...  \n",
       "4     [-4.11558723449707, -1.5119941234588623, -0.96...  \n",
       "...                                                 ...  \n",
       "4027  [-6.796856880187988, -2.6333279609680176, -2.9...  \n",
       "4028  [-7.761892318725586, -2.237544536590576, -1.62...  \n",
       "4029  [-7.654887676239014, -2.525331735610962, -2.34...  \n",
       "4030  [-7.427134990692139, -2.856992721557617, -1.80...  \n",
       "4031  [-7.912234306335449, -2.7759504318237305, -2.0...  \n",
       "\n",
       "[5796 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.analyzer import Analyzer\n",
    "from src.visualizer import VisualisationConfig, visualize\n",
    "from src.metrics import pairwise_bce_of_group\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "logprob_data_path = \"data/logprobs.csv\"\n",
    "pythia_data_path = \"data/pythia_logprobs.csv\"\n",
    "\n",
    "# merge logprobs.csv and pythia_logprobs.csv\n",
    "logprobs = pd.read_csv(logprob_data_path)\n",
    "pythia_logprobs = pd.read_csv(pythia_data_path)\n",
    "\n",
    "# merge the two dataframes\n",
    "logprobs = pd.concat([logprobs, pythia_logprobs])\n",
    "\n",
    "\n",
    "data_analyzer = Analyzer(logprobs)\n",
    "data_analyzer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns, models and adding model family and size columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['EleutherAI/pythia-12b', 'EleutherAI/pythia-160m',\n",
       "        'EleutherAI/pythia-1b', 'EleutherAI/pythia-6.9b',\n",
       "        'meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.2-1B',\n",
       "        'meta-llama/Llama-3.2-3B', 'openai-community/gpt2',\n",
       "        'openai-community/gpt2-large', 'openai-community/gpt2-medium',\n",
       "        'openai-community/gpt2-xl'], dtype=object),\n",
       " array(['{\"revision\": \"step143000\"}', '{\"revision\": \"step33000\"}',\n",
       "        '{\"revision\": \"step66000\"}', '{\"revision\": \"step99000\"}', '{}'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_analyzer.df[\"model_name\"]), np.unique(data_analyzer.df[\"model_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_index</th>\n",
       "      <th>class_type</th>\n",
       "      <th>class</th>\n",
       "      <th>class_elicitation</th>\n",
       "      <th>evidence_category</th>\n",
       "      <th>evidence_text</th>\n",
       "      <th>evidence_elicitation</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>Language Model</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>...</th>\n",
       "      <th>likelihood_prompt</th>\n",
       "      <th>posterior_prompt</th>\n",
       "      <th>prior_full_text</th>\n",
       "      <th>likelihood_full_text</th>\n",
       "      <th>posterior_full_text</th>\n",
       "      <th>prior_token_logprobs</th>\n",
       "      <th>likelihood_token_logprobs</th>\n",
       "      <th>posterior_token_logprobs</th>\n",
       "      <th>Model Family</th>\n",
       "      <th>Model Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>literary_analysis</td>\n",
       "      <td>works that bring out the contemporary social ...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>Llama3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-5.289000511169434, -2.5782995223999023, -6.3...</td>\n",
       "      <td>[-3.8323044776916504, -0.995391845703125, -1.3...</td>\n",
       "      <td>Llama 3</td>\n",
       "      <td>8.030000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>literary_analysis</td>\n",
       "      <td>character-driven narratives.</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>Llama3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-10.583301544189453, -1.8038426637649536, -3....</td>\n",
       "      <td>[-4.018360137939453, -1.8006072044372559, -0.8...</td>\n",
       "      <td>Llama 3</td>\n",
       "      <td>8.030000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>historical_context</td>\n",
       "      <td>literature from periods of significant social...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>Llama3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-4.2250213623046875, -2.044250965118408, -7.6...</td>\n",
       "      <td>[-4.17518424987793, -1.5934702157974243, -1.92...</td>\n",
       "      <td>Llama 3</td>\n",
       "      <td>8.030000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>historical_context</td>\n",
       "      <td>social observers.</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>Llama3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-10.307802200317383, -12.089680671691895, -2....</td>\n",
       "      <td>[-4.350062847137451, -2.5824971199035645, -0.8...</td>\n",
       "      <td>Llama 3</td>\n",
       "      <td>8.030000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>novelists</td>\n",
       "      <td>William Shakespeare.</td>\n",
       "      <td>My favourite author is</td>\n",
       "      <td>cultural_impact</td>\n",
       "      <td>books that challenged conventional thinking a...</td>\n",
       "      <td>I prefer reading</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>Llama3.1-8B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>We've been discussing literary styles and hist...</td>\n",
       "      <td>[-3.5479869842529297, -0.7962633371353149, -0....</td>\n",
       "      <td>[-3.743206024169922, -2.3469152450561523, -11....</td>\n",
       "      <td>[-4.11558723449707, -1.5119941234588623, -0.96...</td>\n",
       "      <td>Llama 3</td>\n",
       "      <td>8.030000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>financial_insight</td>\n",
       "      <td>innovative financing solutions enable more fl...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-11.379182815551758, -12.664813995361328, -6....</td>\n",
       "      <td>[-6.796856880187988, -2.6333279609680176, -2.9...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>travel_experience</td>\n",
       "      <td>immersive cultural interactions create the mo...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-9.444840431213379, -0.29459497332572937, -8....</td>\n",
       "      <td>[-7.761892318725586, -2.237544536590576, -1.62...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>travel_experience</td>\n",
       "      <td>discovering off-the-beaten-path destinations ...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-12.130977630615234, -2.654900312423706, -0.3...</td>\n",
       "      <td>[-7.654887676239014, -2.525331735610962, -2.34...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>fitness_regimen</td>\n",
       "      <td>active travel, such as hiking and cycling, is...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-13.474319458007812, -2.316763401031494, -1.7...</td>\n",
       "      <td>[-7.427134990692139, -2.856992721557617, -1.80...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>7</td>\n",
       "      <td>travel_preferences</td>\n",
       "      <td>luxury travel.</td>\n",
       "      <td>My travel style is best described as</td>\n",
       "      <td>fitness_regimen</td>\n",
       "      <td>integrating fitness routines into travel enha...</td>\n",
       "      <td>I find that traveling by</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>hf</td>\n",
       "      <td>...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>We've been sharing our memorable journeys and ...</td>\n",
       "      <td>[-6.5105414390563965, -3.3153741359710693, -3....</td>\n",
       "      <td>[-11.553764343261719, -8.011931419372559, -7.4...</td>\n",
       "      <td>[-7.912234306335449, -2.7759504318237305, -2.0...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5796 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_index          class_type                  class  \\\n",
       "0              0           novelists   William Shakespeare.   \n",
       "1              0           novelists   William Shakespeare.   \n",
       "2              0           novelists   William Shakespeare.   \n",
       "3              0           novelists   William Shakespeare.   \n",
       "4              0           novelists   William Shakespeare.   \n",
       "...          ...                 ...                    ...   \n",
       "4027           7  travel_preferences         luxury travel.   \n",
       "4028           7  travel_preferences         luxury travel.   \n",
       "4029           7  travel_preferences         luxury travel.   \n",
       "4030           7  travel_preferences         luxury travel.   \n",
       "4031           7  travel_preferences         luxury travel.   \n",
       "\n",
       "                          class_elicitation   evidence_category  \\\n",
       "0                    My favourite author is   literary_analysis   \n",
       "1                    My favourite author is   literary_analysis   \n",
       "2                    My favourite author is  historical_context   \n",
       "3                    My favourite author is  historical_context   \n",
       "4                    My favourite author is     cultural_impact   \n",
       "...                                     ...                 ...   \n",
       "4027   My travel style is best described as   financial_insight   \n",
       "4028   My travel style is best described as   travel_experience   \n",
       "4029   My travel style is best described as   travel_experience   \n",
       "4030   My travel style is best described as     fitness_regimen   \n",
       "4031   My travel style is best described as     fitness_regimen   \n",
       "\n",
       "                                          evidence_text  \\\n",
       "0      works that bring out the contemporary social ...   \n",
       "1                          character-driven narratives.   \n",
       "2      literature from periods of significant social...   \n",
       "3                                     social observers.   \n",
       "4      books that challenged conventional thinking a...   \n",
       "...                                                 ...   \n",
       "4027   innovative financing solutions enable more fl...   \n",
       "4028   immersive cultural interactions create the mo...   \n",
       "4029   discovering off-the-beaten-path destinations ...   \n",
       "4030   active travel, such as hiking and cycling, is...   \n",
       "4031   integrating fitness routines into travel enha...   \n",
       "\n",
       "           evidence_elicitation  \\\n",
       "0              I prefer reading   \n",
       "1              I prefer reading   \n",
       "2              I prefer reading   \n",
       "3              I prefer reading   \n",
       "4              I prefer reading   \n",
       "...                         ...   \n",
       "4027   I find that traveling by   \n",
       "4028   I find that traveling by   \n",
       "4029   I find that traveling by   \n",
       "4030   I find that traveling by   \n",
       "4031   I find that traveling by   \n",
       "\n",
       "                                   conversation_history Language Model  \\\n",
       "0     We've been discussing literary styles and hist...    Llama3.1-8B   \n",
       "1     We've been discussing literary styles and hist...    Llama3.1-8B   \n",
       "2     We've been discussing literary styles and hist...    Llama3.1-8B   \n",
       "3     We've been discussing literary styles and hist...    Llama3.1-8B   \n",
       "4     We've been discussing literary styles and hist...    Llama3.1-8B   \n",
       "...                                                 ...            ...   \n",
       "4027  We've been sharing our memorable journeys and ...     Pythia-12B   \n",
       "4028  We've been sharing our memorable journeys and ...     Pythia-12B   \n",
       "4029  We've been sharing our memorable journeys and ...     Pythia-12B   \n",
       "4030  We've been sharing our memorable journeys and ...     Pythia-12B   \n",
       "4031  We've been sharing our memorable journeys and ...     Pythia-12B   \n",
       "\n",
       "     model_provider  ...                                  likelihood_prompt  \\\n",
       "0                hf  ...  We've been discussing literary styles and hist...   \n",
       "1                hf  ...  We've been discussing literary styles and hist...   \n",
       "2                hf  ...  We've been discussing literary styles and hist...   \n",
       "3                hf  ...  We've been discussing literary styles and hist...   \n",
       "4                hf  ...  We've been discussing literary styles and hist...   \n",
       "...             ...  ...                                                ...   \n",
       "4027             hf  ...  We've been sharing our memorable journeys and ...   \n",
       "4028             hf  ...  We've been sharing our memorable journeys and ...   \n",
       "4029             hf  ...  We've been sharing our memorable journeys and ...   \n",
       "4030             hf  ...  We've been sharing our memorable journeys and ...   \n",
       "4031             hf  ...  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                       posterior_prompt  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                        prior_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                   likelihood_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                    posterior_full_text  \\\n",
       "0     We've been discussing literary styles and hist...   \n",
       "1     We've been discussing literary styles and hist...   \n",
       "2     We've been discussing literary styles and hist...   \n",
       "3     We've been discussing literary styles and hist...   \n",
       "4     We've been discussing literary styles and hist...   \n",
       "...                                                 ...   \n",
       "4027  We've been sharing our memorable journeys and ...   \n",
       "4028  We've been sharing our memorable journeys and ...   \n",
       "4029  We've been sharing our memorable journeys and ...   \n",
       "4030  We've been sharing our memorable journeys and ...   \n",
       "4031  We've been sharing our memorable journeys and ...   \n",
       "\n",
       "                                   prior_token_logprobs  \\\n",
       "0     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "1     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "2     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "3     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "4     [-3.5479869842529297, -0.7962633371353149, -0....   \n",
       "...                                                 ...   \n",
       "4027  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4028  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4029  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4030  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "4031  [-6.5105414390563965, -3.3153741359710693, -3....   \n",
       "\n",
       "                              likelihood_token_logprobs  \\\n",
       "0     [-5.289000511169434, -2.5782995223999023, -6.3...   \n",
       "1     [-10.583301544189453, -1.8038426637649536, -3....   \n",
       "2     [-4.2250213623046875, -2.044250965118408, -7.6...   \n",
       "3     [-10.307802200317383, -12.089680671691895, -2....   \n",
       "4     [-3.743206024169922, -2.3469152450561523, -11....   \n",
       "...                                                 ...   \n",
       "4027  [-11.379182815551758, -12.664813995361328, -6....   \n",
       "4028  [-9.444840431213379, -0.29459497332572937, -8....   \n",
       "4029  [-12.130977630615234, -2.654900312423706, -0.3...   \n",
       "4030  [-13.474319458007812, -2.316763401031494, -1.7...   \n",
       "4031  [-11.553764343261719, -8.011931419372559, -7.4...   \n",
       "\n",
       "                               posterior_token_logprobs  Model Family  \\\n",
       "0     [-3.8323044776916504, -0.995391845703125, -1.3...       Llama 3   \n",
       "1     [-4.018360137939453, -1.8006072044372559, -0.8...       Llama 3   \n",
       "2     [-4.17518424987793, -1.5934702157974243, -1.92...       Llama 3   \n",
       "3     [-4.350062847137451, -2.5824971199035645, -0.8...       Llama 3   \n",
       "4     [-4.11558723449707, -1.5119941234588623, -0.96...       Llama 3   \n",
       "...                                                 ...           ...   \n",
       "4027  [-6.796856880187988, -2.6333279609680176, -2.9...        Pythia   \n",
       "4028  [-7.761892318725586, -2.237544536590576, -1.62...        Pythia   \n",
       "4029  [-7.654887676239014, -2.525331735610962, -2.34...        Pythia   \n",
       "4030  [-7.427134990692139, -2.856992721557617, -1.80...        Pythia   \n",
       "4031  [-7.912234306335449, -2.7759504318237305, -2.0...        Pythia   \n",
       "\n",
       "        Model Size  \n",
       "0     8.030000e+09  \n",
       "1     8.030000e+09  \n",
       "2     8.030000e+09  \n",
       "3     8.030000e+09  \n",
       "4     8.030000e+09  \n",
       "...            ...  \n",
       "4027  1.200000e+10  \n",
       "4028  1.200000e+10  \n",
       "4029  1.200000e+10  \n",
       "4030  1.200000e+10  \n",
       "4031  1.200000e+10  \n",
       "\n",
       "[5796 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_analyzer.rename({\"model_name\": \"Language Model\", \"model_kwargs\": \"Training Steps\"})\n",
    "\n",
    "data_analyzer.add_column(\n",
    "    column_name=\"Model Family\",\n",
    "    column_spec=(\n",
    "        {\"Language Model\": lambda model_name: model_name.split(\"/\")[1].split(\"-\")[0]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def format_model_kwargs(kwarg_str):\n",
    "    if '\"revision\": \"step' in kwarg_str:\n",
    "        # Extract the step number\n",
    "        step = kwarg_str.split(\"step\")[1].split('\"')[0]\n",
    "        # Convert to k format (e.g., 33000 -> 33k)\n",
    "        step_k = str(int(step) // 1000) + \"k\"\n",
    "        return step_k\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "data_analyzer.rename(\n",
    "    {\n",
    "        \"Model Family\": {\n",
    "            \"Llama\": \"Llama 3\",\n",
    "            \"gpt2\": \"GPT 2\",\n",
    "            \"pythia\": \"Pythia\",\n",
    "        },\n",
    "        \"Language Model\": {\n",
    "            \"openai-community/gpt2\": \"GPT-2-S\",\n",
    "            \"openai-community/gpt2-medium\": \"GPT-2-M\",\n",
    "            \"openai-community/gpt2-large\": \"GPT-2-L\",\n",
    "            \"openai-community/gpt2-xl\": \"GPT-2-XL\",\n",
    "            \"meta-llama/Llama-3.2-1B\": \"Llama3.2-1B\",\n",
    "            \"meta-llama/Llama-3.2-3B\": \"Llama3.2-3B\",\n",
    "            \"meta-llama/Llama-3.1-8B\": \"Llama3.1-8B\",\n",
    "            \"EleutherAI/pythia-160m\": \"Pythia-160M\",\n",
    "            \"EleutherAI/pythia-1b\": \"Pythia-1B\",\n",
    "            \"EleutherAI/pythia-6.9b\": \"Pythia-6.9B\",\n",
    "            \"EleutherAI/pythia-12b\": \"Pythia-12B\",\n",
    "        },\n",
    "        \"Training Steps\": format_model_kwargs,\n",
    "    }\n",
    ")\n",
    "\n",
    "data_analyzer.add_column(\n",
    "    column_name=\"Model Size\",\n",
    "    column_spec=(\n",
    "        {\n",
    "            \"Language Model\": {\n",
    "                \"GPT-2-S\": 1.24e8,\n",
    "                \"GPT-2-M\": 3.55e8,\n",
    "                \"GPT-2-L\": 7.74e8,\n",
    "                \"GPT-2-XL\": 1.5e9,\n",
    "                \"Llama3.2-1B\": 1.23e9,\n",
    "                \"Llama3.2-3B\": 3.21e9,\n",
    "                \"Llama3.1-8B\": 8.03e9,\n",
    "                \"Pythia-160M\": 1.6e8,\n",
    "                \"Pythia-1B\": 1.0e9,\n",
    "                \"Pythia-6.9B\": 6.9e9,\n",
    "                \"Pythia-12B\": 1.2e10,\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "data_analyzer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating BCE (both MSE and MAE methods)\n",
    "\n",
    "We always group by class_type since different class types have different class and evidence elicitation prefixes.\n",
    "\n",
    "The calculate_metric functions are unique in that they return an Analyzer object rather than modifying the dataframe in place. This allows calculating multiple metrics on the same dataframe. Since the returned object is an Analyzer, we can continue to filter, sort, rename and add columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>Language Model</th>\n",
       "      <th>Training Steps</th>\n",
       "      <th>BCE (Pairwise MSE)</th>\n",
       "      <th>item_index</th>\n",
       "      <th>class_elicitation</th>\n",
       "      <th>evidence_elicitation</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>temperature</th>\n",
       "      <th>device</th>\n",
       "      <th>model_params</th>\n",
       "      <th>prior_prompt</th>\n",
       "      <th>Model Family</th>\n",
       "      <th>Model Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>architectural_styles</td>\n",
       "      <td>GPT-2-S</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1038.178381</td>\n",
       "      <td>1</td>\n",
       "      <td>My favourite architectural style is</td>\n",
       "      <td>I prefer an architectural style</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 64}</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>GPT 2</td>\n",
       "      <td>1.240000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>architectural_styles</td>\n",
       "      <td>GPT-2-S</td>\n",
       "      <td>unknown</td>\n",
       "      <td>17076.819063</td>\n",
       "      <td>1</td>\n",
       "      <td>My favourite architectural style is</td>\n",
       "      <td>I prefer an architectural style</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 64}</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>GPT 2</td>\n",
       "      <td>1.240000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>architectural_styles</td>\n",
       "      <td>GPT-2-S</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1178.671082</td>\n",
       "      <td>1</td>\n",
       "      <td>My favourite architectural style is</td>\n",
       "      <td>I prefer an architectural style</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 64}</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>GPT 2</td>\n",
       "      <td>1.240000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>architectural_styles</td>\n",
       "      <td>GPT-2-S</td>\n",
       "      <td>unknown</td>\n",
       "      <td>27.390266</td>\n",
       "      <td>1</td>\n",
       "      <td>My favourite architectural style is</td>\n",
       "      <td>I prefer an architectural style</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 64}</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>GPT 2</td>\n",
       "      <td>1.240000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>architectural_styles</td>\n",
       "      <td>GPT-2-S</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1161.371513</td>\n",
       "      <td>1</td>\n",
       "      <td>My favourite architectural style is</td>\n",
       "      <td>I prefer an architectural style</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 64}</td>\n",
       "      <td>We've been discussing European architectural s...</td>\n",
       "      <td>GPT 2</td>\n",
       "      <td>1.240000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104685</th>\n",
       "      <td>writing_style</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>143k</td>\n",
       "      <td>3.094117</td>\n",
       "      <td>3</td>\n",
       "      <td>I feel my writing style is in a particular wr...</td>\n",
       "      <td>Boutta hit the store</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 32}</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104686</th>\n",
       "      <td>writing_style</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>143k</td>\n",
       "      <td>12.053169</td>\n",
       "      <td>3</td>\n",
       "      <td>I feel my writing style is in a particular wr...</td>\n",
       "      <td>Boutta hit the store</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 32}</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104687</th>\n",
       "      <td>writing_style</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>143k</td>\n",
       "      <td>252.215507</td>\n",
       "      <td>3</td>\n",
       "      <td>I feel my writing style is in a particular wr...</td>\n",
       "      <td>Boutta hit the store</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 32}</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104688</th>\n",
       "      <td>writing_style</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>143k</td>\n",
       "      <td>309.550643</td>\n",
       "      <td>3</td>\n",
       "      <td>I feel my writing style is in a particular wr...</td>\n",
       "      <td>Boutta hit the store</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 32}</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104689</th>\n",
       "      <td>writing_style</td>\n",
       "      <td>Pythia-12B</td>\n",
       "      <td>143k</td>\n",
       "      <td>2.933537</td>\n",
       "      <td>3</td>\n",
       "      <td>I feel my writing style is in a particular wr...</td>\n",
       "      <td>Boutta hit the store</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>hf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{\"batch_size\": 32}</td>\n",
       "      <td>We've been discussing different writing styles...</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1.200000e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106490 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  class_type Language Model Training Steps  \\\n",
       "1560    architectural_styles        GPT-2-S        unknown   \n",
       "1561    architectural_styles        GPT-2-S        unknown   \n",
       "1562    architectural_styles        GPT-2-S        unknown   \n",
       "1563    architectural_styles        GPT-2-S        unknown   \n",
       "1564    architectural_styles        GPT-2-S        unknown   \n",
       "...                      ...            ...            ...   \n",
       "104685         writing_style     Pythia-12B           143k   \n",
       "104686         writing_style     Pythia-12B           143k   \n",
       "104687         writing_style     Pythia-12B           143k   \n",
       "104688         writing_style     Pythia-12B           143k   \n",
       "104689         writing_style     Pythia-12B           143k   \n",
       "\n",
       "        BCE (Pairwise MSE)  item_index  \\\n",
       "1560           1038.178381           1   \n",
       "1561          17076.819063           1   \n",
       "1562           1178.671082           1   \n",
       "1563             27.390266           1   \n",
       "1564           1161.371513           1   \n",
       "...                    ...         ...   \n",
       "104685            3.094117           3   \n",
       "104686           12.053169           3   \n",
       "104687          252.215507           3   \n",
       "104688          309.550643           3   \n",
       "104689            2.933537           3   \n",
       "\n",
       "                                        class_elicitation  \\\n",
       "1560                  My favourite architectural style is   \n",
       "1561                  My favourite architectural style is   \n",
       "1562                  My favourite architectural style is   \n",
       "1563                  My favourite architectural style is   \n",
       "1564                  My favourite architectural style is   \n",
       "...                                                   ...   \n",
       "104685   I feel my writing style is in a particular wr...   \n",
       "104686   I feel my writing style is in a particular wr...   \n",
       "104687   I feel my writing style is in a particular wr...   \n",
       "104688   I feel my writing style is in a particular wr...   \n",
       "104689   I feel my writing style is in a particular wr...   \n",
       "\n",
       "                    evidence_elicitation  \\\n",
       "1560     I prefer an architectural style   \n",
       "1561     I prefer an architectural style   \n",
       "1562     I prefer an architectural style   \n",
       "1563     I prefer an architectural style   \n",
       "1564     I prefer an architectural style   \n",
       "...                                  ...   \n",
       "104685              Boutta hit the store   \n",
       "104686              Boutta hit the store   \n",
       "104687              Boutta hit the store   \n",
       "104688              Boutta hit the store   \n",
       "104689              Boutta hit the store   \n",
       "\n",
       "                                     conversation_history model_provider  \\\n",
       "1560    We've been discussing European architectural s...             hf   \n",
       "1561    We've been discussing European architectural s...             hf   \n",
       "1562    We've been discussing European architectural s...             hf   \n",
       "1563    We've been discussing European architectural s...             hf   \n",
       "1564    We've been discussing European architectural s...             hf   \n",
       "...                                                   ...            ...   \n",
       "104685  We've been discussing different writing styles...             hf   \n",
       "104686  We've been discussing different writing styles...             hf   \n",
       "104687  We've been discussing different writing styles...             hf   \n",
       "104688  We've been discussing different writing styles...             hf   \n",
       "104689  We've been discussing different writing styles...             hf   \n",
       "\n",
       "        temperature device        model_params  \\\n",
       "1560            1.0   cuda  {\"batch_size\": 64}   \n",
       "1561            1.0   cuda  {\"batch_size\": 64}   \n",
       "1562            1.0   cuda  {\"batch_size\": 64}   \n",
       "1563            1.0   cuda  {\"batch_size\": 64}   \n",
       "1564            1.0   cuda  {\"batch_size\": 64}   \n",
       "...             ...    ...                 ...   \n",
       "104685          1.0   cuda  {\"batch_size\": 32}   \n",
       "104686          1.0   cuda  {\"batch_size\": 32}   \n",
       "104687          1.0   cuda  {\"batch_size\": 32}   \n",
       "104688          1.0   cuda  {\"batch_size\": 32}   \n",
       "104689          1.0   cuda  {\"batch_size\": 32}   \n",
       "\n",
       "                                             prior_prompt Model Family  \\\n",
       "1560    We've been discussing European architectural s...        GPT 2   \n",
       "1561    We've been discussing European architectural s...        GPT 2   \n",
       "1562    We've been discussing European architectural s...        GPT 2   \n",
       "1563    We've been discussing European architectural s...        GPT 2   \n",
       "1564    We've been discussing European architectural s...        GPT 2   \n",
       "...                                                   ...          ...   \n",
       "104685  We've been discussing different writing styles...       Pythia   \n",
       "104686  We've been discussing different writing styles...       Pythia   \n",
       "104687  We've been discussing different writing styles...       Pythia   \n",
       "104688  We've been discussing different writing styles...       Pythia   \n",
       "104689  We've been discussing different writing styles...       Pythia   \n",
       "\n",
       "          Model Size  \n",
       "1560    1.240000e+08  \n",
       "1561    1.240000e+08  \n",
       "1562    1.240000e+08  \n",
       "1563    1.240000e+08  \n",
       "1564    1.240000e+08  \n",
       "...              ...  \n",
       "104685  1.200000e+10  \n",
       "104686  1.200000e+10  \n",
       "104687  1.200000e+10  \n",
       "104688  1.200000e+10  \n",
       "104689  1.200000e+10  \n",
       "\n",
       "[106490 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_mse_analyzer = data_analyzer.calculate_metric(\n",
    "    metric_name=\"BCE (Pairwise MSE)\",\n",
    "    metric_func=pairwise_bce_of_group,\n",
    "    group_by_cols=[\"class_type\", \"Language Model\", \"Training Steps\"],\n",
    "    log_prior_col=\"prior_logprob\",\n",
    "    log_likelihood_col=\"likelihood_logprob\",\n",
    "    log_posterior_col=\"posterior_logprob\",\n",
    "    square=True,\n",
    "    inherit_identical_values=True,\n",
    ")\n",
    "\n",
    "bce_mae_analyzer = data_analyzer.calculate_metric(\n",
    "    metric_name=\"BCE (Pairwise MAE)\",\n",
    "    metric_func=pairwise_bce_of_group,\n",
    "    group_by_cols=[\"class_type\", \"Language Model\", \"Training Steps\"],\n",
    "    log_prior_col=\"prior_logprob\",\n",
    "    log_likelihood_col=\"likelihood_logprob\",\n",
    "    log_posterior_col=\"posterior_logprob\",\n",
    "    square=False,\n",
    "    inherit_identical_values=True,\n",
    ")\n",
    "\n",
    "# Set sort order using the *new* names\n",
    "for analyzer in [bce_mse_analyzer, bce_mae_analyzer]:\n",
    "    analyzer.sort(\n",
    "        {\n",
    "            \"Language Model\": [\n",
    "                \"GPT-2-S\",\n",
    "                \"GPT-2-M\",\n",
    "                \"GPT-2-L\",\n",
    "                \"GPT-2-XL\",\n",
    "                \"Llama3.2-1B\",\n",
    "                \"Llama3.2-3B\",\n",
    "                \"Llama3.1-8B\",\n",
    "                \"Pythia-160M\",\n",
    "                \"Pythia-1B\",\n",
    "                \"Pythia-6.9B\",\n",
    "                \"Pythia-12B\",\n",
    "            ],\n",
    "            \"Training Steps\": [\n",
    "                \"33k\",\n",
    "                \"66k\",\n",
    "                \"99k\",\n",
    "                \"143k\",\n",
    "                \"unknown\",\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "bce_mse_analyzer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a visualization config and visualize the data\n",
    "\n",
    "src.visualizer provides a very simple interface for quickly visualizing lots of data by describing which column to map to which plotting element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f6a7379b0a1045d09c3c74a100923840.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f6a7379b0a1045d09c3c74a100923840.vega-embed details,\n",
       "  #altair-viz-f6a7379b0a1045d09c3c74a100923840.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f6a7379b0a1045d09c3c74a100923840\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f6a7379b0a1045d09c3c74a100923840\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f6a7379b0a1045d09c3c74a100923840\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"Language Model\": \"GPT-2-S\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MSE)\": 1824.9760863909892, \"__count\": 4630}, {\"Language Model\": \"GPT-2-M\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MSE)\": 1532.3144635681533, \"__count\": 4630}, {\"Language Model\": \"GPT-2-L\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MSE)\": 1506.0610348465882, \"__count\": 4630}, {\"Language Model\": \"GPT-2-XL\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MSE)\": 1380.6924069472416, \"__count\": 4630}, {\"Language Model\": \"Llama3.2-1B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MSE)\": 1229.5050758066184, \"__count\": 4630}, {\"Language Model\": \"Llama3.2-3B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MSE)\": 1142.527269479485, \"__count\": 4630}, {\"Language Model\": \"Llama3.1-8B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MSE)\": 1045.948171375305, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1877.3820178912988, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1901.1354479463155, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1954.5934149179254, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1931.4177656066493, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1571.3842943623663, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1477.5834362935336, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1492.8014981948181, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1380.2485709507707, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1293.7509008904212, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1342.7411140697025, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1323.1377630159081, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1268.0666964150298, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1349.8362793780325, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1310.9012328219885, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1262.12200814483, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MSE)\": 1155.4585732293565, \"__count\": 4630}]}, {\"name\": \"facet_domain\"}, {\"name\": \"facet_domain_row\", \"values\": [{\"data\": 0.0}]}, {\"name\": \"source_0_y_domain_mean_BCE (Pairwise MSE)\", \"values\": [{\"min\": 1045.948171375305, \"max\": 1954.5934149179254}]}, {\"name\": \"source_0_color_domain_Training Steps\", \"values\": [{\"Training Steps\": \"unknown\"}, {\"Training Steps\": \"33k\"}, {\"Training Steps\": \"66k\"}, {\"Training Steps\": \"99k\"}, {\"Training Steps\": \"143k\"}]}], \"signals\": [{\"name\": \"child_x_step\", \"value\": 20}, {\"name\": \"child_height\", \"value\": 300}], \"marks\": [{\"type\": \"group\", \"name\": \"facet-title\", \"title\": {\"text\": \"Model Family:N\", \"offset\": 10, \"style\": \"guide-title\"}, \"role\": \"column-title\"}, {\"type\": \"group\", \"name\": \"row_header\", \"from\": {\"data\": \"facet_domain_row\"}, \"encode\": {\"update\": {\"height\": {\"signal\": \"child_height\"}}}, \"axes\": [{\"scale\": \"y\", \"orient\": \"left\", \"title\": \"Mean BCE (Pairwise MSE method)\", \"labelOverlap\": true, \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}, \"grid\": false, \"zindex\": 0}], \"role\": \"row-header\"}, {\"type\": \"group\", \"name\": \"cell\", \"from\": {\"facet\": {\"data\": \"source_0\", \"name\": \"facet\", \"groupby\": [\"Model Family\"], \"aggregate\": {\"fields\": [\"Language Model\"], \"ops\": [\"distinct\"], \"as\": [\"distinct_Language Model\"]}}}, \"sort\": {\"field\": [\"datum[\\\"Model Family\\\"]\"], \"order\": [\"ascending\"]}, \"encode\": {\"update\": {\"height\": {\"signal\": \"child_height\"}, \"width\": {\"signal\": \"bandspace(datum[\\\"distinct_Language Model\\\"], 1, 0.5) * child_x_step\"}}}, \"data\": [{\"name\": \"facet_child_x_domain_Language Model_2\", \"source\": \"facet\", \"transform\": [{\"type\": \"aggregate\", \"groupby\": [\"Language Model\"], \"fields\": [], \"ops\": [], \"as\": []}, {\"type\": \"formula\", \"expr\": \"datum['Language Model']\", \"as\": \"sort_field\"}, {\"type\": \"project\", \"fields\": [\"Language Model\"]}]}], \"marks\": [{\"type\": \"group\", \"name\": \"child_pathgroup\", \"from\": {\"facet\": {\"data\": \"facet\", \"name\": \"faceted_path_child_main\", \"groupby\": [\"Training Steps\"]}}, \"encode\": {\"update\": {\"width\": {\"field\": {\"signal\": null, \"datum\": null, \"group\": \"width\", \"parent\": null}}, \"height\": {\"field\": {\"signal\": null, \"datum\": null, \"group\": \"height\", \"parent\": null}}}}, \"marks\": [{\"type\": \"line\", \"name\": \"child_marks\", \"from\": {\"data\": \"faceted_path_child_main\"}, \"sort\": {\"field\": \"x\"}, \"encode\": {\"update\": {\"tooltip\": {\"signal\": \"{\\\"Model\\\": isValid(datum[\\\"Language Model\\\"]) ? datum[\\\"Language Model\\\"] : \\\"\\\"+datum[\\\"Language Model\\\"], \\\"Family\\\": isValid(datum[\\\"Model Family\\\"]) ? datum[\\\"Model Family\\\"] : \\\"\\\"+datum[\\\"Model Family\\\"], \\\"Mean BCE\\\": format(datum[\\\"mean_BCE (Pairwise MSE)\\\"], \\\".3f\\\"), \\\"Count\\\": format(datum[\\\"__count\\\"], \\\"d\\\")}\"}, \"x\": {\"field\": \"Language Model\", \"scale\": \"child_x\"}, \"stroke\": {\"field\": \"Training Steps\", \"scale\": \"color\"}, \"y\": {\"field\": \"mean_BCE (Pairwise MSE)\", \"scale\": \"y\"}, \"defined\": {\"signal\": \"isValid(datum[\\\"mean_BCE (Pairwise MSE)\\\"]) && isFinite(+datum[\\\"mean_BCE (Pairwise MSE)\\\"])\"}}}, \"style\": [\"line\"]}]}], \"scales\": [{\"name\": \"child_x\", \"type\": \"point\", \"domain\": {\"data\": \"facet_child_x_domain_Language Model_2\", \"field\": \"Language Model\"}, \"range\": {\"step\": {\"signal\": \"child_x_step\"}}, \"padding\": 0.5}], \"axes\": [{\"scale\": \"y\", \"maxExtent\": 0, \"minExtent\": 0, \"grid\": true, \"zindex\": 0, \"labels\": false, \"orient\": \"left\", \"aria\": false, \"gridScale\": \"child_x\", \"domain\": false, \"ticks\": false, \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}}, {\"scale\": \"child_x\", \"orient\": \"bottom\", \"labelAlign\": \"right\", \"labelAngle\": 270, \"labelBaseline\": \"middle\", \"zindex\": 0, \"grid\": false, \"title\": \"Language Model:N\"}], \"title\": {\"text\": {\"signal\": \"isValid(parent[\\\"Model Family\\\"]) ? parent[\\\"Model Family\\\"] : \\\"\\\"+parent[\\\"Model Family\\\"]\"}, \"frame\": \"group\", \"offset\": 10, \"style\": \"guide-label\"}, \"style\": \"cell\"}], \"scales\": [{\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MSE)\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MSE)\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"child_height\"}, 0], \"nice\": true, \"zero\": true}, {\"name\": \"color\", \"type\": \"ordinal\", \"domain\": {\"data\": \"source_0_color_domain_Training Steps\", \"field\": \"Training Steps\"}, \"range\": \"category\"}], \"title\": {\"text\": \"BCE (Pairwise MSE method) by Model\", \"anchor\": \"start\"}, \"padding\": 5, \"background\": \"white\", \"legends\": [{\"columns\": 5, \"orient\": \"bottom\", \"title\": \"Training Steps:N\", \"stroke\": \"color\", \"direction\": \"horizontal\", \"symbolType\": \"stroke\"}], \"layout\": {\"padding\": 20, \"bounds\": \"full\", \"align\": \"none\", \"columns\": 3}}, {\"mode\": \"vega\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = VisualisationConfig(\n",
    "    plot_fn=alt.Chart.mark_line,\n",
    "    fig_title=\"BCE (Pairwise MSE method) by Model\",\n",
    "    x_category=\"Language Model:N\",\n",
    "    y_category=\"mean(BCE (Pairwise MSE)):Q\",\n",
    "    color_category=\"Training Steps:N\",\n",
    "    facet_category=\"Model Family:N\",\n",
    "    tooltip_fields=[\n",
    "        alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "        alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "        alt.Tooltip(\"mean(BCE (Pairwise MSE)):Q\", title=\"Mean BCE\", format=\".3f\"),\n",
    "        alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "        alt.Tooltip(\"mean():Q\", title=\"Mean\", format=\".3f\"),\n",
    "        alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "    ],\n",
    "    titles={\n",
    "        \"mean(BCE (Pairwise MSE)):Q\": \"Mean BCE (Pairwise MSE method)\",\n",
    "        \"model_name\": \"Language Model\",\n",
    "    },\n",
    "    chart_properties={\"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}}},\n",
    "    legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    ")\n",
    "\n",
    "chart_mse = visualize(bce_mse_analyzer.df, config=config)\n",
    "\n",
    "chart_mse.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-e582ade236514855a72c791f2f42a45e.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-e582ade236514855a72c791f2f42a45e.vega-embed details,\n",
       "  #altair-viz-e582ade236514855a72c791f2f42a45e.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-e582ade236514855a72c791f2f42a45e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e582ade236514855a72c791f2f42a45e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e582ade236514855a72c791f2f42a45e\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"Language Model\": \"GPT-2-S\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MAE)\": 25.658118358029146, \"__count\": 4630}, {\"Language Model\": \"GPT-2-M\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MAE)\": 24.057437740803795, \"__count\": 4630}, {\"Language Model\": \"GPT-2-L\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MAE)\": 24.076395289583804, \"__count\": 4630}, {\"Language Model\": \"GPT-2-XL\", \"Training Steps\": \"unknown\", \"Model Family\": \"GPT 2\", \"mean_BCE (Pairwise MAE)\": 23.534624254677773, \"__count\": 4630}, {\"Language Model\": \"Llama3.2-1B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MAE)\": 21.977195783413254, \"__count\": 4630}, {\"Language Model\": \"Llama3.2-3B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MAE)\": 21.07376566669596, \"__count\": 4630}, {\"Language Model\": \"Llama3.1-8B\", \"Training Steps\": \"unknown\", \"Model Family\": \"Llama 3\", \"mean_BCE (Pairwise MAE)\": 20.488662858555177, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 25.75452889016071, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 26.372443589690437, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 26.100557535307455, \"__count\": 4630}, {\"Language Model\": \"Pythia-160M\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 25.46988517656182, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 23.63532353590683, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 23.274875738811286, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 23.220138547590434, \"__count\": 4630}, {\"Language Model\": \"Pythia-1B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.65864338153897, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 21.749587561656796, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 23.006747958109138, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.76982964444624, \"__count\": 4630}, {\"Language Model\": \"Pythia-6.9B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.4051069418246, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"33k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.4201001369155, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"66k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.444931294233186, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"99k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 22.118546627149726, \"__count\": 4630}, {\"Language Model\": \"Pythia-12B\", \"Training Steps\": \"143k\", \"Model Family\": \"Pythia\", \"mean_BCE (Pairwise MAE)\": 21.482912399521656, \"__count\": 4630}]}, {\"name\": \"facet_domain\"}, {\"name\": \"facet_domain_row\", \"values\": [{\"data\": 0.0}]}, {\"name\": \"source_0_y_domain_mean_BCE (Pairwise MAE)\", \"values\": [{\"min\": 20.488662858555177, \"max\": 26.372443589690437}]}, {\"name\": \"source_0_color_domain_Training Steps\", \"values\": [{\"Training Steps\": \"unknown\"}, {\"Training Steps\": \"33k\"}, {\"Training Steps\": \"66k\"}, {\"Training Steps\": \"99k\"}, {\"Training Steps\": \"143k\"}]}], \"signals\": [{\"name\": \"child_x_step\", \"value\": 20}, {\"name\": \"child_height\", \"value\": 300}], \"marks\": [{\"type\": \"group\", \"name\": \"facet-title\", \"title\": {\"text\": \"Model Family:N\", \"style\": \"guide-title\", \"offset\": 10}, \"role\": \"column-title\"}, {\"type\": \"group\", \"name\": \"row_header\", \"from\": {\"data\": \"facet_domain_row\"}, \"encode\": {\"update\": {\"height\": {\"signal\": \"child_height\"}}}, \"axes\": [{\"scale\": \"y\", \"title\": \"Mean BCE (Pairwise MAE method)\", \"orient\": \"left\", \"zindex\": 0, \"grid\": false, \"labelOverlap\": true, \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}}], \"role\": \"row-header\"}, {\"type\": \"group\", \"name\": \"cell\", \"from\": {\"facet\": {\"data\": \"source_0\", \"name\": \"facet\", \"groupby\": [\"Model Family\"], \"aggregate\": {\"fields\": [\"Language Model\"], \"ops\": [\"distinct\"], \"as\": [\"distinct_Language Model\"]}}}, \"sort\": {\"field\": [\"datum[\\\"Model Family\\\"]\"], \"order\": [\"ascending\"]}, \"encode\": {\"update\": {\"width\": {\"signal\": \"bandspace(datum[\\\"distinct_Language Model\\\"], 1, 0.5) * child_x_step\"}, \"height\": {\"signal\": \"child_height\"}}}, \"data\": [{\"name\": \"facet_child_x_domain_Language Model_2\", \"source\": \"facet\", \"transform\": [{\"type\": \"aggregate\", \"groupby\": [\"Language Model\"], \"fields\": [], \"ops\": [], \"as\": []}, {\"type\": \"formula\", \"expr\": \"datum['Language Model']\", \"as\": \"sort_field\"}, {\"type\": \"project\", \"fields\": [\"Language Model\"]}]}], \"marks\": [{\"type\": \"group\", \"name\": \"child_pathgroup\", \"from\": {\"facet\": {\"data\": \"facet\", \"name\": \"faceted_path_child_main\", \"groupby\": [\"Training Steps\"]}}, \"encode\": {\"update\": {\"height\": {\"field\": {\"signal\": null, \"datum\": null, \"group\": \"height\", \"parent\": null}}, \"width\": {\"field\": {\"signal\": null, \"datum\": null, \"group\": \"width\", \"parent\": null}}}}, \"marks\": [{\"type\": \"line\", \"name\": \"child_marks\", \"from\": {\"data\": \"faceted_path_child_main\"}, \"sort\": {\"field\": \"x\"}, \"encode\": {\"update\": {\"x\": {\"field\": \"Language Model\", \"scale\": \"child_x\"}, \"stroke\": {\"field\": \"Training Steps\", \"scale\": \"color\"}, \"tooltip\": {\"signal\": \"{\\\"Model\\\": isValid(datum[\\\"Language Model\\\"]) ? datum[\\\"Language Model\\\"] : \\\"\\\"+datum[\\\"Language Model\\\"], \\\"Family\\\": isValid(datum[\\\"Model Family\\\"]) ? datum[\\\"Model Family\\\"] : \\\"\\\"+datum[\\\"Model Family\\\"], \\\"Mean BCE\\\": format(datum[\\\"mean_BCE (Pairwise MAE)\\\"], \\\".3f\\\"), \\\"Count\\\": format(datum[\\\"__count\\\"], \\\"d\\\")}\"}, \"defined\": {\"signal\": \"isValid(datum[\\\"mean_BCE (Pairwise MAE)\\\"]) && isFinite(+datum[\\\"mean_BCE (Pairwise MAE)\\\"])\"}, \"y\": {\"field\": \"mean_BCE (Pairwise MAE)\", \"scale\": \"y\"}}}, \"style\": [\"line\"]}]}], \"scales\": [{\"name\": \"child_x\", \"type\": \"point\", \"domain\": {\"data\": \"facet_child_x_domain_Language Model_2\", \"field\": \"Language Model\"}, \"range\": {\"step\": {\"signal\": \"child_x_step\"}}, \"padding\": 0.5}], \"axes\": [{\"scale\": \"y\", \"ticks\": false, \"maxExtent\": 0, \"zindex\": 0, \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}, \"domain\": false, \"orient\": \"left\", \"grid\": true, \"gridScale\": \"child_x\", \"aria\": false, \"labels\": false, \"minExtent\": 0}, {\"scale\": \"child_x\", \"title\": \"Language Model:N\", \"grid\": false, \"labelBaseline\": \"middle\", \"zindex\": 0, \"labelAlign\": \"right\", \"labelAngle\": 270, \"orient\": \"bottom\"}], \"title\": {\"text\": {\"signal\": \"isValid(parent[\\\"Model Family\\\"]) ? parent[\\\"Model Family\\\"] : \\\"\\\"+parent[\\\"Model Family\\\"]\"}, \"style\": \"guide-label\", \"frame\": \"group\", \"offset\": 10}, \"style\": \"cell\"}], \"scales\": [{\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MAE)\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MAE)\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"child_height\"}, 0], \"zero\": true, \"nice\": true}, {\"name\": \"color\", \"type\": \"ordinal\", \"domain\": {\"data\": \"source_0_color_domain_Training Steps\", \"field\": \"Training Steps\"}, \"range\": \"category\"}], \"title\": {\"text\": \"BCE (Pairwise MAE method) by Model\", \"anchor\": \"start\"}, \"padding\": 5, \"layout\": {\"padding\": 20, \"bounds\": \"full\", \"align\": \"none\", \"columns\": 3}, \"legends\": [{\"columns\": 5, \"orient\": \"bottom\", \"title\": \"Training Steps:N\", \"stroke\": \"color\", \"direction\": \"horizontal\", \"symbolType\": \"stroke\"}], \"background\": \"white\"}, {\"mode\": \"vega\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = VisualisationConfig(\n",
    "    plot_fn=alt.Chart.mark_line,\n",
    "    fig_title=\"BCE (Pairwise MAE method) by Model\",\n",
    "    x_category=\"Language Model:N\",\n",
    "    y_category=\"mean(BCE (Pairwise MAE)):Q\",\n",
    "    color_category=\"Training Steps:N\",\n",
    "    facet_category=\"Model Family:N\",\n",
    "    tooltip_fields=[\n",
    "        alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "        alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "        alt.Tooltip(\"mean(BCE (Pairwise MAE)):Q\", title=\"Mean BCE\", format=\".3f\"),\n",
    "        alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "        alt.Tooltip(\"mean():Q\", title=\"Mean\", format=\".3f\"),\n",
    "        alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "    ],\n",
    "    titles={\n",
    "        \"mean(BCE (Pairwise MAE)):Q\": \"Mean BCE (Pairwise MAE method)\",\n",
    "        \"model_name\": \"Language Model\",\n",
    "    },\n",
    "    chart_properties={\"resolve\": {\"scale\": {\"x\": \"independent\"}}},\n",
    "    legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    ")\n",
    "\n",
    "chart_mse = visualize(bce_mae_analyzer.df, config=config)\n",
    "\n",
    "chart_mse.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['143k', 'unknown'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for analyzer in [bce_mse_analyzer, bce_mae_analyzer]:\n",
    "    analyzer.filter({\"Training Steps\": [\"unknown\", \"143k\"]})\n",
    "\n",
    "np.unique(bce_mse_analyzer.df[\"Training Steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-18a6724718db492abcf5d2b57d9702fe.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-18a6724718db492abcf5d2b57d9702fe.vega-embed details,\n",
       "  #altair-viz-18a6724718db492abcf5d2b57d9702fe.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-18a6724718db492abcf5d2b57d9702fe\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-18a6724718db492abcf5d2b57d9702fe\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-18a6724718db492abcf5d2b57d9702fe\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"Language Model\": \"GPT-2-S\", \"Model Family\": \"GPT 2\", \"Model Size\": 124000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1824.9760863909892}, {\"Language Model\": \"GPT-2-M\", \"Model Family\": \"GPT 2\", \"Model Size\": 355000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1532.3144635681533}, {\"Language Model\": \"GPT-2-L\", \"Model Family\": \"GPT 2\", \"Model Size\": 774000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1506.0610348465882}, {\"Language Model\": \"GPT-2-XL\", \"Model Family\": \"GPT 2\", \"Model Size\": 1500000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1380.6924069472416}, {\"Language Model\": \"Llama3.2-1B\", \"Model Family\": \"Llama 3\", \"Model Size\": 1230000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1229.5050758066184}, {\"Language Model\": \"Llama3.2-3B\", \"Model Family\": \"Llama 3\", \"Model Size\": 3210000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1142.527269479485}, {\"Language Model\": \"Llama3.1-8B\", \"Model Family\": \"Llama 3\", \"Model Size\": 8030000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1045.948171375305}, {\"Language Model\": \"Pythia-160M\", \"Model Family\": \"Pythia\", \"Model Size\": 160000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1931.417765606645}, {\"Language Model\": \"Pythia-1B\", \"Model Family\": \"Pythia\", \"Model Size\": 1000000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1380.2485709507685}, {\"Language Model\": \"Pythia-6.9B\", \"Model Family\": \"Pythia\", \"Model Size\": 6900000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1268.0666964150298}, {\"Language Model\": \"Pythia-12B\", \"Model Family\": \"Pythia\", \"Model Size\": 12000000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MSE)\": 1155.4585732293588}]}, {\"name\": \"source_0_y_domain_mean_BCE (Pairwise MSE)\", \"values\": [{\"min\": 1045.948171375305, \"max\": 1931.417765606645}]}, {\"name\": \"source_0_color_domain_Model Family\", \"values\": [{\"Model Family\": \"GPT 2\"}, {\"Model Family\": \"Llama 3\"}, {\"Model Family\": \"Pythia\"}]}], \"marks\": [{\"type\": \"symbol\", \"name\": \"marks\", \"from\": {\"data\": \"source_0\"}, \"encode\": {\"update\": {\"x\": {\"field\": \"Model Size\", \"scale\": \"x\"}, \"stroke\": {\"field\": \"Model Family\", \"scale\": \"color\"}, \"y\": {\"field\": \"mean_BCE (Pairwise MSE)\", \"scale\": \"y\"}, \"fill\": {\"value\": \"transparent\"}, \"tooltip\": {\"signal\": \"{\\\"Model\\\": isValid(datum[\\\"Language Model\\\"]) ? datum[\\\"Language Model\\\"] : \\\"\\\"+datum[\\\"Language Model\\\"], \\\"Family\\\": isValid(datum[\\\"Model Family\\\"]) ? datum[\\\"Model Family\\\"] : \\\"\\\"+datum[\\\"Model Family\\\"], \\\"Mean BCE\\\": format(datum[\\\"mean_BCE (Pairwise MSE)\\\"], \\\".3f\\\"), \\\"Count\\\": format(datum[\\\"__count\\\"], \\\"d\\\")}\"}}}, \"style\": [\"point\"]}], \"scales\": [{\"name\": \"x\", \"type\": \"log\", \"domain\": {\"data\": \"source_0\", \"field\": \"Model Size\"}, \"range\": [0, {\"signal\": \"width\"}], \"nice\": true}, {\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MSE)\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MSE)\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"height\"}, 0], \"zero\": false, \"nice\": true}, {\"name\": \"color\", \"type\": \"ordinal\", \"domain\": {\"data\": \"source_0_color_domain_Model Family\", \"field\": \"Model Family\"}, \"range\": \"category\"}], \"axes\": [{\"scale\": \"x\", \"domain\": false, \"maxExtent\": 0, \"grid\": true, \"zindex\": 0, \"ticks\": false, \"minExtent\": 0, \"labels\": false, \"orient\": \"bottom\", \"aria\": false, \"gridScale\": \"y\"}, {\"scale\": \"y\", \"domain\": false, \"aria\": false, \"grid\": true, \"labels\": false, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"orient\": \"left\", \"maxExtent\": 0, \"gridScale\": \"x\", \"ticks\": false, \"minExtent\": 0, \"zindex\": 0}, {\"scale\": \"x\", \"zindex\": 0, \"title\": \"Model Size:Q\", \"orient\": \"bottom\", \"labelFlush\": true, \"labelOverlap\": \"greedy\", \"grid\": false}, {\"scale\": \"y\", \"zindex\": 0, \"grid\": false, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"title\": \"Mean BCE (Pairwise MSE method)\", \"orient\": \"left\", \"labelOverlap\": true}], \"title\": {\"text\": \"BCE (Pairwise MSE method) by Model\", \"frame\": \"group\"}, \"legends\": [{\"columns\": 5, \"orient\": \"bottom\", \"title\": \"Model Family:N\", \"stroke\": \"color\", \"direction\": \"horizontal\", \"symbolType\": \"circle\", \"encode\": {\"symbols\": {\"update\": {\"fill\": {\"value\": \"transparent\"}}}}}], \"style\": \"cell\", \"width\": 300, \"padding\": 5, \"background\": \"white\", \"height\": 300}, {\"mode\": \"vega\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = VisualisationConfig(\n",
    "    plot_fn=alt.Chart.mark_point,\n",
    "    fig_title=\"BCE (Pairwise MSE method) by Model\",\n",
    "    x_category=\"Model Size:Q\",\n",
    "    y_category=\"mean(BCE (Pairwise MSE)):Q\",\n",
    "    color_category=\"Model Family:N\",\n",
    "    tooltip_fields=[\n",
    "        alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "        alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "        alt.Tooltip(\"mean(BCE (Pairwise MSE)):Q\", title=\"Mean BCE\", format=\".3f\"),\n",
    "        alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "        alt.Tooltip(\"mean():Q\", title=\"Mean\", format=\".3f\"),\n",
    "        alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "    ],\n",
    "    titles={\n",
    "        \"mean(BCE (Pairwise MSE)):Q\": \"Mean BCE (Pairwise MSE method)\",\n",
    "        \"model_name\": \"Language Model\",\n",
    "    },\n",
    "    scale={\n",
    "        \"Model Size:Q\": {\"type\": \"log\"},\n",
    "        \"mean(BCE (Pairwise MSE)):Q\": {\"zero\": False},\n",
    "    },\n",
    "    chart_properties={\n",
    "        \"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}},\n",
    "    },\n",
    "    legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    ")\n",
    "\n",
    "chart_mse = visualize(bce_mse_analyzer.df, config=config)\n",
    "\n",
    "chart_mse.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d.vega-embed details,\n",
       "  #altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d6917e6dce7747f4b8f65c19d1c1a83d\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"Language Model\": \"GPT-2-S\", \"Model Family\": \"GPT 2\", \"Model Size\": 124000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 25.658118358029146}, {\"Language Model\": \"GPT-2-M\", \"Model Family\": \"GPT 2\", \"Model Size\": 355000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 24.057437740803795}, {\"Language Model\": \"GPT-2-L\", \"Model Family\": \"GPT 2\", \"Model Size\": 774000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 24.076395289583804}, {\"Language Model\": \"GPT-2-XL\", \"Model Family\": \"GPT 2\", \"Model Size\": 1500000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 23.534624254677773}, {\"Language Model\": \"Llama3.2-1B\", \"Model Family\": \"Llama 3\", \"Model Size\": 1230000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 21.977195783413254}, {\"Language Model\": \"Llama3.2-3B\", \"Model Family\": \"Llama 3\", \"Model Size\": 3210000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 21.07376566669596}, {\"Language Model\": \"Llama3.1-8B\", \"Model Family\": \"Llama 3\", \"Model Size\": 8030000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 20.488662858555177}, {\"Language Model\": \"Pythia-160M\", \"Model Family\": \"Pythia\", \"Model Size\": 160000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 25.46988517656182}, {\"Language Model\": \"Pythia-1B\", \"Model Family\": \"Pythia\", \"Model Size\": 1000000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 22.65864338153897}, {\"Language Model\": \"Pythia-6.9B\", \"Model Family\": \"Pythia\", \"Model Size\": 6900000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 22.4051069418246}, {\"Language Model\": \"Pythia-12B\", \"Model Family\": \"Pythia\", \"Model Size\": 12000000000.0, \"__count\": 4630, \"mean_BCE (Pairwise MAE)\": 21.482912399521656}]}, {\"name\": \"source_0_y_domain_mean_BCE (Pairwise MAE)\", \"values\": [{\"min\": 20.488662858555177, \"max\": 25.658118358029146}]}, {\"name\": \"source_0_color_domain_Model Family\", \"values\": [{\"Model Family\": \"GPT 2\"}, {\"Model Family\": \"Llama 3\"}, {\"Model Family\": \"Pythia\"}]}], \"marks\": [{\"type\": \"symbol\", \"name\": \"marks\", \"from\": {\"data\": \"source_0\"}, \"encode\": {\"update\": {\"stroke\": {\"field\": \"Model Family\", \"scale\": \"color\"}, \"y\": {\"field\": \"mean_BCE (Pairwise MAE)\", \"scale\": \"y\"}, \"tooltip\": {\"signal\": \"{\\\"Model\\\": isValid(datum[\\\"Language Model\\\"]) ? datum[\\\"Language Model\\\"] : \\\"\\\"+datum[\\\"Language Model\\\"], \\\"Family\\\": isValid(datum[\\\"Model Family\\\"]) ? datum[\\\"Model Family\\\"] : \\\"\\\"+datum[\\\"Model Family\\\"], \\\"Mean BCE\\\": format(datum[\\\"mean_BCE (Pairwise MAE)\\\"], \\\".3f\\\"), \\\"Count\\\": format(datum[\\\"__count\\\"], \\\"d\\\")}\"}, \"x\": {\"field\": \"Model Size\", \"scale\": \"x\"}, \"fill\": {\"value\": \"transparent\"}}}, \"style\": [\"point\"]}], \"scales\": [{\"name\": \"x\", \"type\": \"log\", \"domain\": {\"data\": \"source_0\", \"field\": \"Model Size\"}, \"range\": [0, {\"signal\": \"width\"}], \"nice\": true}, {\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MAE)\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain_mean_BCE (Pairwise MAE)\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"height\"}, 0], \"zero\": false, \"nice\": true}, {\"name\": \"color\", \"type\": \"ordinal\", \"domain\": {\"data\": \"source_0_color_domain_Model Family\", \"field\": \"Model Family\"}, \"range\": \"category\"}], \"axes\": [{\"scale\": \"x\", \"minExtent\": 0, \"zindex\": 0, \"grid\": true, \"labels\": false, \"aria\": false, \"domain\": false, \"gridScale\": \"y\", \"maxExtent\": 0, \"orient\": \"bottom\", \"ticks\": false}, {\"scale\": \"y\", \"gridScale\": \"x\", \"orient\": \"left\", \"aria\": false, \"maxExtent\": 0, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"domain\": false, \"zindex\": 0, \"grid\": true, \"labels\": false, \"ticks\": false, \"minExtent\": 0}, {\"scale\": \"x\", \"zindex\": 0, \"title\": \"Model Size:Q\", \"labelFlush\": true, \"labelOverlap\": \"greedy\", \"orient\": \"bottom\", \"grid\": false}, {\"scale\": \"y\", \"orient\": \"left\", \"grid\": false, \"labelOverlap\": true, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"zindex\": 0, \"title\": \"Mean BCE (Pairwise MAE method)\"}], \"title\": {\"text\": \"BCE (Pairwise MAE method) by Model\", \"frame\": \"group\"}, \"padding\": 5, \"style\": \"cell\", \"height\": 300, \"background\": \"white\", \"width\": 300, \"legends\": [{\"columns\": 5, \"orient\": \"bottom\", \"title\": \"Model Family:N\", \"stroke\": \"color\", \"direction\": \"horizontal\", \"symbolType\": \"circle\", \"encode\": {\"symbols\": {\"update\": {\"fill\": {\"value\": \"transparent\"}}}}}]}, {\"mode\": \"vega\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = VisualisationConfig(\n",
    "    plot_fn=alt.Chart.mark_point,\n",
    "    fig_title=\"BCE (Pairwise MAE method) by Model\",\n",
    "    x_category=\"Model Size:Q\",\n",
    "    y_category=\"mean(BCE (Pairwise MAE)):Q\",\n",
    "    color_category=\"Model Family:N\",\n",
    "    tooltip_fields=[\n",
    "        alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "        alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "        alt.Tooltip(\"mean(BCE (Pairwise MAE)):Q\", title=\"Mean BCE\", format=\".3f\"),\n",
    "        alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "        alt.Tooltip(\"mean():Q\", title=\"Mean\", format=\".3f\"),\n",
    "        alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "    ],\n",
    "    titles={\n",
    "        \"mean(BCE (Pairwise MAE)):Q\": \"Mean BCE (Pairwise MAE method)\",\n",
    "        \"model_name\": \"Language Model\",\n",
    "    },\n",
    "    scale={\n",
    "        \"Model Size:Q\": {\"type\": \"log\"},\n",
    "        \"mean(BCE (Pairwise MAE)):Q\": {\"zero\": False},\n",
    "    },\n",
    "    chart_properties={\n",
    "        \"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}},\n",
    "    },\n",
    "    legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    ")\n",
    "\n",
    "chart_mse = visualize(bce_mae_analyzer.df, config=config)\n",
    "\n",
    "chart_mse.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
