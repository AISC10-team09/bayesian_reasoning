{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing how coherence scales with model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analyzer import Analyzer\n",
    "from src.visualizer import VisualisationConfig, visualize\n",
    "from src.metrics import pairwise_bce_of_group, nbce_of_group, cbc_of_group, scs_of_group, rbc_of_group\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "logprob_data_paths = [\"data/logprobs.csv\"]\n",
    "\n",
    "# merge logprobs.csv and pythia_logprobs.csv\n",
    "\n",
    "logprobs = [pd.read_csv(logprob_data_path) for logprob_data_path in logprob_data_paths]\n",
    "\n",
    "\n",
    "# merge the two dataframes\n",
    "logprobs = pd.concat(logprobs)\n",
    "logprobs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "data_analyzer = Analyzer(logprobs)\n",
    "data_analyzer.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzer.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns, models and adding model family and size columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_analyzer.df[\"model_name\"]), np.unique(data_analyzer.df[\"model_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals from open-llm-leaderboard\n",
    "evals_df = pd.read_parquet(\n",
    "    \"hf://datasets/open-llm-leaderboard/contents/data/train-00000-of-00001.parquet\"\n",
    ")\n",
    "evals_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzer.rename({\"model_name\": \"Language Model\", \"model_kwargs\": \"Training Steps\"})\n",
    "\n",
    "params_lookup = (\n",
    "    \n",
    "    evals_df.drop_duplicates(subset=[\"fullname\"])\n",
    "    .set_index(\"fullname\")[\"#Params (B)\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Add the '#Params (B)' column to data_analyzer.df\n",
    "data_analyzer.add_column(\n",
    "    column_name=\"#Params (B)\",\n",
    "    column_spec=lambda df: df[\"Language Model\"]\n",
    "    .map(params_lookup)\n",
    "    .astype(float),  # Ensure it's float for numerical operations\n",
    ")\n",
    "\n",
    "selected_evals = [\n",
    "    \"IFEval\",\n",
    "    \"BBH\",\n",
    "    \"MATH Lvl 5\",\n",
    "    \"GPQA\",\n",
    "    \"MUSR\",\n",
    "    \"MMLU-PRO\",\n",
    "    \"MMLU-PRO\",\n",
    "    \"Average \u2b06\ufe0f\",\n",
    "]\n",
    "\n",
    "# Create a lookup dictionary once for efficiency\n",
    "eval_lookup = {}\n",
    "for model_name in evals_df[\"fullname\"].unique():\n",
    "    eval_lookup[model_name] = {\n",
    "        eval_name: evals_df[evals_df[\"fullname\"] == model_name][eval_name].iloc[0]\n",
    "        for eval_name in selected_evals\n",
    "        if not evals_df[evals_df[\"fullname\"] == model_name].empty\n",
    "    }\n",
    "\n",
    "# Add columns using direct dictionary lookups\n",
    "for eval in selected_evals:\n",
    "    data_analyzer.add_column(\n",
    "        column_name=eval,\n",
    "        column_spec=lambda df, eval_name=eval: df[\"Language Model\"].map(\n",
    "            lambda model: eval_lookup.get(model, {}).get(eval_name, np.nan)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "data_analyzer.add_column(\n",
    "    column_name=\"Model Family\",\n",
    "    column_spec=(\n",
    "        {\"Language Model\": lambda model_name: model_name.split(\"/\")[1].split(\"-\")[0]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def format_model_kwargs(kwarg_str):\n",
    "    if '\"revision\": \"step' in kwarg_str:\n",
    "        # Extract the step number\n",
    "        step = kwarg_str.split(\"step\")[1].split('\"')[0]\n",
    "        # Convert to k format (e.g., 33000 -> 33k)\n",
    "        step_k = str(int(step) // 1000) + \"k\"\n",
    "        return step_k\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    \n",
    "\n",
    "data_analyzer.filter({\"Model Family\": [\"Llama\", \"gpt2\", \"pythia\", \"Qwen2.5\", \"Falcon3\"]})\n",
    "\n",
    "data_analyzer.rename(\n",
    "    {\n",
    "        \"Model Family\": {\n",
    "            \"Llama\": \"Llama 3\",\n",
    "            \"gpt2\": \"GPT 2\",\n",
    "            \"pythia\": \"Pythia\",\n",
    "            \"Qwen2.5\": \"Qwen 2.5\",\n",
    "        },\n",
    "        \"Language Model\": lambda model_name: model_name.split(\"/\")[1],\n",
    "        \"Training Steps\": format_model_kwargs,\n",
    "    }\n",
    ")\n",
    "\n",
    "data_analyzer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating All Coherence Metrics\n",
    "\n",
    "We calculate all coherence metrics systematically using the same grouping strategy. We always group by evidence_text since different class types have different class and evidence elicitation prefixes.\n",
    "\n",
    "The calculate_metric functions return Analyzer objects rather than modifying the dataframe in place, allowing us to chain operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all metrics to calculate\n",
    "metrics_config = [\n",
    "    {\"name\": \"BCE\", \"func\": pairwise_bce_of_group, \"kwargs\": {\"square\": True}, \"display_name\": \"BCE (Pairwise MSE)\"},\n",
    "    {\"name\": \"NBCE\", \"func\": nbce_of_group, \"kwargs\": {}, \"display_name\": \"NBCE\"},\n",
    "    {\"name\": \"CBC\", \"func\": cbc_of_group, \"kwargs\": {}, \"display_name\": \"CBC\"},\n",
    "    {\"name\": \"SCS\", \"func\": scs_of_group, \"kwargs\": {}, \"display_name\": \"SCS\"},\n",
    "    {\"name\": \"RBC\", \"func\": rbc_of_group, \"kwargs\": {}, \"display_name\": \"RBC\"}\n",
    "]\n",
    "\n",
    "# Calculate all metrics using the same structure\n",
    "analyzers = {}\n",
    "group_by_cols = [\n",
    "    \"evidence_text\",\n",
    "    \"class_category\",\n",
    "    \"Language Model\", \n",
    "    \"Training Steps\",\n",
    "    \"conversation_history\"\n",
    "]\n",
    "\n",
    "sort_config = {\n",
    "    \"#Params (B)\": lambda x: x,\n",
    "    \"Training Steps\": [\"33k\", \"66k\", \"99k\", \"143k\", \"unknown\"],\n",
    "}\n",
    "\n",
    "for metric_config in metrics_config:\n",
    "    analyzer = data_analyzer.calculate_metric(\n",
    "        metric_name=metric_config[\"display_name\"],\n",
    "        metric_func=metric_config[\"func\"],\n",
    "        group_by_cols=group_by_cols,\n",
    "        log_prior_col=\"prior_logprob\",\n",
    "        log_likelihood_col=\"likelihood_logprob\",\n",
    "        log_posterior_col=\"posterior_logprob\",\n",
    "        inherit_identical_values=True,\n",
    "        **metric_config[\"kwargs\"]\n",
    "    )\n",
    "    \n",
    "    analyzer.sort(sort_config)\n",
    "    analyzers[metric_config[\"name\"]] = analyzer\n",
    "\n",
    "print(\"Calculated metrics:\")\n",
    "for name, analyzer in analyzers.items():\n",
    "    print(f\"{name}: {analyzer.df.shape}\")\n",
    "\n",
    "# For backwards compatibility\n",
    "bce_mse_analyzer = analyzers[\"BCE\"]\n",
    "bce_mse_analyzer.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Calculate Mean Values for All Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean values for all metrics systematically\n",
    "mean_analyzers = {}\n",
    "\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    \n",
    "    mean_analyzer = analyzers[name].calculate_metric(\n",
    "        metric_func=\"mean\",\n",
    "        group_by_cols=[\"Language Model\"],\n",
    "        metric_col=display_name,\n",
    "        metric_name=f\"Mean {display_name}\",\n",
    "        inherit_identical_values=True,\n",
    "    )\n",
    "    \n",
    "    mean_analyzers[name] = mean_analyzer\n",
    "\n",
    "print(\"Mean analyzers created:\")\n",
    "for name, analyzer in mean_analyzers.items():\n",
    "    print(f\"{name}: {analyzer.df.shape}\")\n",
    "\n",
    "# For backwards compatibility\n",
    "mean_bce_mse_analyzer = mean_analyzers[\"BCE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzers['NBCE'].df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Systematic Visualization of All Metrics\n",
    "\n",
    "Create consistent visualizations for all metrics using the visualizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for each metric using the visualizer\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    analyzer = analyzers[name]\n",
    "    \n",
    "    config = VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_line,\n",
    "        fig_title=f\"{name} by Model\",\n",
    "        x_category=\"Language Model:N\",\n",
    "        y_category=f\"mean({display_name}):Q\",\n",
    "        color_category=\"Training Steps:N\",\n",
    "        facet_category=\"Model Family:N\",\n",
    "        facet_columns=10,\n",
    "        tooltip_fields=[\n",
    "            alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "            alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "            alt.Tooltip(f\"mean({display_name}):Q\", title=f\"Mean {name}\", format=\".3f\"),\n",
    "            alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "            alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "        ],\n",
    "        titles={\n",
    "            f\"mean({display_name}):Q\": f\"Mean {name}\",\n",
    "        },\n",
    "        chart_properties={\"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}}},\n",
    "        legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n=== {name} ===\")\n",
    "    chart = visualize(analyzer.df, config=config)\n",
    "    chart.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Calculate mean values for all metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Comparative Analysis of All Metrics\n",
    "\n",
    "Let's create comprehensive comparison plots for all metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all mean metrics into a single dataframe for comparison\n",
    "combined_data = []\n",
    "\n",
    "metric_cols = {\n",
    "    \"BCE\": \"Mean BCE (Pairwise MSE)\",\n",
    "    \"NBCE\": \"Mean NBCE\", \n",
    "    \"CBC\": \"Mean CBC\",\n",
    "    \"SCS\": \"Mean SCS\",\n",
    "    \"RBC\": \"Mean RBC\"\n",
    "}\n",
    "\n",
    "# Create a combined dataset\n",
    "for metric_name, analyzer in mean_analyzers.items():\n",
    "    df_copy = analyzer.df.copy()\n",
    "    df_copy[\"Metric\"] = metric_name\n",
    "    df_copy[\"Value\"] = df_copy[metric_cols[metric_name]]\n",
    "    combined_data.append(df_copy[[\"Language Model\", \"Model Family\", \"#Params (B)\", \"Metric\", \"Value\"]])\n",
    "\n",
    "combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "# Filter out unknown training steps for cleaner comparison  \n",
    "combined_df = combined_df.dropna(subset=[\"Value\"])\n",
    "\n",
    "print(\"Combined dataframe shape:\", combined_df.shape)\n",
    "print(\"\\\\nMetrics included:\", combined_df[\"Metric\"].unique())\n",
    "print(\"\\\\nModels included:\", combined_df[\"Language Model\"].unique())\n",
    "\n",
    "# Display sample of the combined data\n",
    "combined_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a visualization config and visualize the data\n",
    "\n",
    "src.visualizer provides a very simple interface for quickly visualizing lots of data by describing which column to map to which plotting element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VisualisationConfig(\n",
    "    plot_fn=alt.Chart.mark_line,\n",
    "    fig_title=\"BCE (Pairwise MSE method) by Model\",\n",
    "    x_category=\"Language Model:N\",\n",
    "    y_category=\"mean(BCE (Pairwise MSE)):Q\",\n",
    "    color_category=\"Training Steps:N\",\n",
    "    facet_category=\"Model Family:N\",\n",
    "    facet_columns=10,\n",
    "    tooltip_fields=[\n",
    "        alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "        alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "        alt.Tooltip(\"mean(BCE (Pairwise MSE)):Q\", title=\"Mean BCE\", format=\".3f\"),\n",
    "        alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "        alt.Tooltip(\"mean():Q\", title=\"Mean\", format=\".3f\"),\n",
    "        alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "    ],\n",
    "    titles={\n",
    "        \"mean(BCE (Pairwise MSE)):Q\": \"Mean BCE (Pairwise MSE method)\",\n",
    "        \"model_name\": \"Language Model\",\n",
    "    },\n",
    "    chart_properties={\"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}}},\n",
    "    legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    ")\n",
    "\n",
    "chart_mse = visualize(bce_mse_analyzer.df, config=config)\n",
    "\n",
    "chart_mse.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting All Metrics by Model Size\n",
    "\n",
    "Filter all metrics to keep only fully trained models (remove intermediate Pythia checkpoints).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all analyzers to keep only fully trained models\n",
    "for name, analyzer in analyzers.items():\n",
    "    analyzer.filter({\"Training Steps\": [\"unknown\", \"143k\"]})\n",
    "\n",
    "for name, analyzer in mean_analyzers.items():\n",
    "    analyzer.filter({\"Training Steps\": [\"unknown\", \"143k\"]})\n",
    "\n",
    "print(\"Training steps after filtering:\")\n",
    "for name, analyzer in analyzers.items():\n",
    "    print(f\"{name}: {analyzer.df['Training Steps'].unique()}\")\n",
    "\n",
    "# For backwards compatibility\n",
    "bce_mse_analyzer = analyzers[\"BCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPQA plots for all metrics\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    analyzer = analyzers[name]\n",
    "    \n",
    "    config = VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_point,\n",
    "        fig_title=f\"{name} vs GPQA Performance\",\n",
    "        x_category=\"GPQA:Q\",\n",
    "        y_category=f\"mean({display_name}):Q\",\n",
    "        color_category=\"Model Family:N\",\n",
    "        tooltip_fields=[\n",
    "            alt.Tooltip(\"Language Model:N\", title=\"Model\"),\n",
    "            alt.Tooltip(\"Model Family:N\", title=\"Family\"),\n",
    "            alt.Tooltip(f\"mean({display_name}):Q\", title=f\"Mean {name}\", format=\".3f\"),\n",
    "            alt.Tooltip(\"GPQA:Q\", title=\"GPQA\", format=\".3f\"),\n",
    "            alt.Tooltip(\"median():Q\", title=\"Median\", format=\".3f\"),\n",
    "            alt.Tooltip(\"count():Q\", title=\"Count\", format=\"d\"),\n",
    "        ],\n",
    "        titles={\n",
    "            f\"mean({display_name}):Q\": f\"Mean {name}\",\n",
    "        },\n",
    "        scale={\n",
    "            f\"mean({display_name}):Q\": {\"zero\": False},\n",
    "        },\n",
    "        chart_properties={\n",
    "            \"resolve\": {\"scale\": {\"x\": \"independent\", \"y\": \"shared\"}},\n",
    "        },\n",
    "        legend_config={\"columns\": 5, \"orient\": \"bottom\"},\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n=== {name} vs GPQA ===\")\n",
    "    chart = visualize(analyzer.df, config=config)\n",
    "    chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Analysis: Distribution Across Groups\n",
    "\n",
    "Show how different class categories affect coherence for each metric across model families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create systematic visualization and analysis plots for ALL metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Function to fit log-linear model\n",
    "def log_linear_fit(data, x_col, y_col, model_family=None):\n",
    "    if model_family:\n",
    "        data = data[data[\"Model Family\"] == model_family]\n",
    "\n",
    "    x = np.log10(data[x_col])  # Log transform x values\n",
    "    y = data[y_col]\n",
    "\n",
    "    # Fit linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    # Calculate line\n",
    "    x_range = np.linspace(min(x), max(x), 100)\n",
    "    y_range = slope * x_range + intercept\n",
    "\n",
    "    # Transform x back to original scale for plotting\n",
    "    x_range_original = 10**x_range\n",
    "\n",
    "    return x_range_original, y_range, r_value**2, slope\n",
    "\n",
    "# Create comprehensive plots for all metrics\n",
    "print(\"=== SYSTEMATIC PLOTTING FOR ALL METRICS ===\")\n",
    "\n",
    "# 1. Model size scaling plots with lines of best fit\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    mean_analyzer = mean_analyzers[name]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle(f\"{name} Scaling Analysis\", fontsize=16)\n",
    "    \n",
    "    # Plot 1: All families together with lines of best fit\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for model_family in np.unique(mean_analyzer.df[\"Model Family\"]):\n",
    "        family_data = mean_analyzer.df[mean_analyzer.df[\"Model Family\"] == model_family]\n",
    "        plt.scatter(\n",
    "            family_data[\"#Params (B)\"],\n",
    "            family_data[f\"Mean {display_name}\"],\n",
    "            label=model_family,\n",
    "            alpha=0.7,\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        # Add fitted line\n",
    "        if len(family_data) > 1:\n",
    "            x_line, y_line, r2, slope = log_linear_fit(family_data, \"#Params (B)\", f\"Mean {display_name}\")\n",
    "            plt.plot(\n",
    "                x_line,\n",
    "                y_line,\n",
    "                \"--\",\n",
    "                alpha=0.7,\n",
    "                label=f\"{model_family}: R\u00b2 = {r2:.3f}\",\n",
    "            )\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Model Size (params)\")\n",
    "    plt.ylabel(f\"Mean {name}\")\n",
    "    plt.title(f\"{name} vs Model Size\")\n",
    "    plt.legend(fontsize=9, loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: GPQA correlation\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mask = ~np.isnan(mean_analyzer.df[\"GPQA\"]) & ~np.isnan(mean_analyzer.df[f\"Mean {display_name}\"])\n",
    "    if sum(mask) > 1:\n",
    "        x = mean_analyzer.df[\"GPQA\"][mask]\n",
    "        y_data = mean_analyzer.df[f\"Mean {display_name}\"][mask]\n",
    "        \n",
    "        # Scatter plot with model families\n",
    "        for model_family in np.unique(mean_analyzer.df[\"Model Family\"]):\n",
    "            family_mask = mask & (mean_analyzer.df[\"Model Family\"] == model_family)\n",
    "            if sum(family_mask) > 0:\n",
    "                plt.scatter(\n",
    "                    mean_analyzer.df[\"GPQA\"][family_mask],\n",
    "                    mean_analyzer.df[f\"Mean {display_name}\"][family_mask],\n",
    "                    label=model_family,\n",
    "                    alpha=0.7,\n",
    "                    s=60\n",
    "                )\n",
    "        \n",
    "        # Add line of best fit\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y_data)\n",
    "        line_x = np.linspace(min(x), max(x), 100)\n",
    "        line_y = intercept + slope * line_x\n",
    "        plt.plot(line_x, line_y, \"r-\", linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Add confidence interval\n",
    "        if len(x) > 2:\n",
    "            mean_x = np.mean(x)\n",
    "            n = len(x)\n",
    "            df_res = n - 2\n",
    "            mse = np.sum((y_data - (intercept + slope * x)) ** 2) / df_res\n",
    "            se = np.sqrt(mse * (1 + 1 / n + (line_x - mean_x) ** 2 / np.sum((x - mean_x) ** 2)))\n",
    "            t_val = stats.t.ppf(0.975, df_res)\n",
    "            ci = t_val * se\n",
    "            plt.fill_between(line_x, line_y - ci, line_y + ci, color=\"r\", alpha=0.2)\n",
    "        \n",
    "        plt.text(0.05, 0.95, f\"R\u00b2 = {r_value**2:.3f}\", transform=plt.gca().transAxes, \n",
    "                verticalalignment=\"top\", bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.xlabel(\"GPQA Score\")\n",
    "    plt.ylabel(f\"Mean {name}\")\n",
    "    plt.title(f\"{name} vs GPQA Performance\")\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Distribution across model families (box plot)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    family_data_list = []\n",
    "    family_labels = []\n",
    "    for family in np.unique(mean_analyzer.df[\"Model Family\"]):\n",
    "        family_values = mean_analyzer.df[mean_analyzer.df[\"Model Family\"] == family][f\"Mean {display_name}\"]\n",
    "        if len(family_values) > 0:\n",
    "            family_data_list.append(family_values)\n",
    "            family_labels.append(family)\n",
    "    \n",
    "    plt.boxplot(family_data_list, labels=family_labels)\n",
    "    plt.ylabel(f\"Mean {name}\")\n",
    "    plt.title(f\"{name} Distribution by Model Family\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: All evaluation metrics correlation\n",
    "    plt.subplot(2, 2, 4)\n",
    "    eval_metrics = [\"IFEval\", \"BBH\", \"MATH Lvl 5\", \"MUSR\", \"MMLU-PRO\", \"Average \u2b06\ufe0f\"]\n",
    "    correlations = []\n",
    "    eval_names = []\n",
    "    \n",
    "    for eval_metric in eval_metrics:\n",
    "        mask = ~np.isnan(mean_analyzer.df[eval_metric]) & ~np.isnan(mean_analyzer.df[f\"Mean {display_name}\"])\n",
    "        if sum(mask) > 1:\n",
    "            corr, _ = stats.pearsonr(\n",
    "                mean_analyzer.df[eval_metric][mask], \n",
    "                mean_analyzer.df[f\"Mean {display_name}\"][mask]\n",
    "            )\n",
    "            correlations.append(corr)\n",
    "            eval_names.append(eval_metric)\n",
    "    \n",
    "    if correlations:\n",
    "        colors = ['red' if c < 0 else 'green' for c in correlations]\n",
    "        bars = plt.bar(range(len(correlations)), correlations, color=colors, alpha=0.7)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.ylabel(f\"Correlation with {name}\")\n",
    "        plt.title(f\"{name} vs Evaluation Metrics\")\n",
    "        plt.xticks(range(len(eval_names)), eval_names, rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation values on bars\n",
    "        for i, (bar, corr) in enumerate(zip(bars, correlations)):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if corr > 0 else bar.get_height() - 0.03,\n",
    "                    f'{corr:.3f}', ha='center', va='bottom' if corr > 0 else 'top', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"\\\\nCompleted comprehensive analysis for {name}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category analysis for all metrics systematically\n",
    "import seaborn as sns\n",
    "\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    analyzer = analyzers[name]\n",
    "    \n",
    "    print(f\"\\\\n=== {name} Category Analysis ===\")\n",
    "    \n",
    "    # Calculate mean by category for each metric\n",
    "    mean_category_analyzer = analyzer.calculate_metric(\n",
    "        metric_func=\"mean\",\n",
    "        group_by_cols=[\"Language Model\", \"class_category\"],\n",
    "        metric_col=display_name,\n",
    "        metric_name=f\"Mean {display_name}\",\n",
    "        inherit_identical_values=True,\n",
    "    )\n",
    "\n",
    "    # Calculate counts for sample size information\n",
    "    counts = (\n",
    "        analyzer.df\n",
    "        .groupby(['Language Model', 'class_category'])\n",
    "        .size()\n",
    "        .reset_index(name='n')\n",
    "    )\n",
    "\n",
    "    # Merge counts into the mean df\n",
    "    df = mean_category_analyzer.df.copy()\n",
    "    df = df.merge(counts, on=['Language Model', 'class_category'], how='left')\n",
    "\n",
    "    def normalize_category(cat):\n",
    "        return cat.replace('_', ' ').title()\n",
    "\n",
    "    df['category_with_n'] = df['class_category'].apply(normalize_category) + '\\\\n(n=' + df['n'].astype(str) + ')'\n",
    "\n",
    "    # Select the largest model per family for cleaner visualization\n",
    "    largest_models_per_family = mean_analyzers[name].df.groupby('Model Family')['#Params (B)'].idxmax()\n",
    "    largest_model_names = mean_analyzers[name].df.loc[largest_models_per_family]['Language Model'].values\n",
    "    df_filtered = df[df['Language Model'].isin(largest_model_names)]\n",
    "\n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(data=df_filtered, x='category_with_n', y=f'Mean {display_name}', hue='Language Model')\n",
    "    plt.xlabel('Class Category')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(f'{name}')\n",
    "    plt.legend(title='Model (Largest per Family)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(f'{name} Distribution Across Class Categories\\\\n(Largest model per family)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythia Training Steps Analysis: All Metrics\n",
    "\n",
    "Analyze how all coherence metrics change during training for Pythia models, instead of just BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All metrics across training steps (pythia models only)\n",
    "pythia_data_analyzer = data_analyzer\n",
    "\n",
    "pythia_data_analyzer.filter({\"Model Family\": [\"Pythia\"]})\n",
    "\n",
    "print(\"Pythia models and training steps available:\")\n",
    "print(pythia_data_analyzer.df[[\"Training Steps\", \"Language Model\"]].drop_duplicates().sort_values([\"Language Model\", \"Training Steps\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics for Pythia models systematically\n",
    "pythia_metrics = {}\n",
    "metric_col_names = {\n",
    "    \"BCE\": \"BCE (Pairwise MSE)\",\n",
    "    \"NBCE\": \"NBCE\",\n",
    "    \"CBC\": \"CBC\", \n",
    "    \"SCS\": \"SCS\",\n",
    "    \"RBC\": \"RBC\"\n",
    "}\n",
    "\n",
    "for metric_config in metrics_config:\n",
    "    name = metric_config[\"name\"]\n",
    "    display_name = metric_config[\"display_name\"]\n",
    "    \n",
    "    pythia_metrics[name] = pythia_data_analyzer.calculate_metric(\n",
    "        metric_name=display_name,\n",
    "        metric_func=metric_config[\"func\"],\n",
    "        group_by_cols=[\n",
    "            \"evidence_text\",\n",
    "            \"class_category\",\n",
    "            \"Language Model\",\n",
    "            \"Training Steps\",\n",
    "            \"conversation_history\"\n",
    "        ],\n",
    "        log_prior_col=\"prior_logprob\",\n",
    "        log_likelihood_col=\"likelihood_logprob\",\n",
    "        log_posterior_col=\"posterior_logprob\",\n",
    "        inherit_identical_values=True,\n",
    "        **metric_config[\"kwargs\"]\n",
    "    )\n",
    "\n",
    "# Calculate means for all metrics\n",
    "pythia_mean_metrics = {}\n",
    "\n",
    "for metric_name, analyzer in pythia_metrics.items():\n",
    "    pythia_mean_metrics[metric_name] = analyzer.calculate_metric(\n",
    "        metric_func=\"mean\",\n",
    "        group_by_cols=[\"Language Model\", \"Training Steps\"],\n",
    "        metric_col=metric_col_names[metric_name],\n",
    "        metric_name=f\"Mean {metric_col_names[metric_name]}\",\n",
    "        inherit_identical_values=True,\n",
    "    )\n",
    "    \n",
    "    # Set sort order\n",
    "    pythia_mean_metrics[metric_name].sort(\n",
    "        {\n",
    "            \"#Params (B)\": lambda x: x,\n",
    "            \"Training Steps\": [\n",
    "                \"33k\",\n",
    "                \"66k\",\n",
    "                \"99k\",\n",
    "                \"143k\",\n",
    "                \"unknown\",\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Pythia metrics calculated:\")\n",
    "for name, analyzer in pythia_mean_metrics.items():\n",
    "    print(f\"{name}: {analyzer.df.shape}\")\n",
    "\n",
    "# For backwards compatibility\n",
    "mean_pythia_bce_mse_analyzer = pythia_mean_metrics[\"BCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plots for all metrics across training steps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('All Coherence Metrics vs Training Steps (Pythia Models)', fontsize=16)\n",
    "\n",
    "axes = axes.flatten()\n",
    "pythia_metrics_to_plot = [\"BCE\", \"NBCE\", \"CBC\", \"SCS\", \"RBC\"]\n",
    "\n",
    "for i, metric in enumerate(pythia_metrics_to_plot):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get the correct column name for the y-axis\n",
    "    y_col = f\"Mean {metric_col_names[metric]}\"\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=pythia_mean_metrics[metric].df, \n",
    "        x='Training Steps', \n",
    "        y=y_col, \n",
    "        hue='Language Model',\n",
    "        ax=ax,\n",
    "        marker='o'\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel(f'{metric}')\n",
    "    ax.set_title(f'{metric} vs Training Steps')\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[5].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n=== Training Step Analysis Summary ===\")\n",
    "print(\"Changes from 33k to 143k training steps:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for metric in pythia_metrics_to_plot:\n",
    "    print(f\"\\\\n{metric}:\")\n",
    "    pythia_data = pythia_mean_metrics[metric].df\n",
    "    \n",
    "    # Calculate improvement/change from first to last checkpoint\n",
    "    for model in pythia_data[\"Language Model\"].unique():\n",
    "        model_data = pythia_data[pythia_data[\"Language Model\"] == model]\n",
    "        if len(model_data) >= 2:\n",
    "            steps = [\"33k\", \"66k\", \"99k\", \"143k\"]\n",
    "            available_steps = [s for s in steps if s in model_data[\"Training Steps\"].values]\n",
    "            if len(available_steps) >= 2:\n",
    "                first_step = available_steps[0]\n",
    "                last_step = available_steps[-1]\n",
    "                \n",
    "                y_col = f\"Mean {metric_col_names[metric]}\"\n",
    "                first_val = model_data[model_data[\"Training Steps\"] == first_step][y_col].iloc[0]\n",
    "                last_val = model_data[model_data[\"Training Steps\"] == last_step][y_col].iloc[0]\n",
    "                \n",
    "                change = last_val - first_val\n",
    "                percent_change = (change / first_val * 100) if first_val != 0 else 0\n",
    "                \n",
    "                direction = \"\u2191\" if change > 0 else \"\u2193\" if change < 0 else \"\u2192\"\n",
    "                print(f\"  {model}: {first_val:.4f} \u2192 {last_val:.4f} ({direction} {abs(percent_change):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}