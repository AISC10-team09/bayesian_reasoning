{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vLLM library not found. VLLMInterface will not be functional.\n",
      "Using combinations mapping: running each model with each parameter set and each model_kwargs set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Model: meta-llama/Llama-3.1-8B with provider: hf and params: {'batch_size': 32} ---\n",
      "Using HFInterface for meta-llama/Llama-3.1-8B with model_kwargs: {}\n",
      "Initializing LLMInterface for model: meta-llama/Llama-3.1-8B with params: {'model_kwargs': {}, 'batch_size': 32, 'verbose': True}\n",
      "Initializing HFInterface for meta-llama/Llama-3.1-8B\n",
      "Using device: cuda\n",
      "Warning: Tokenizer for meta-llama/Llama-3.1-8B lacks a pad token. Using EOS token (<|end_of_text|>) for padding.\n",
      "Attempting to load model with identifier: 'meta-llama/Llama-3.1-8B'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd67be79dda4aa9867b88ff12633f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3.1-8B loaded successfully on cuda.\n",
      "Generated 9690 prompts for model meta-llama/Llama-3.1-8B with params {'batch_size': 32}. Processing via HFInterface...\n",
      "Using batch size: 32, temperature: 1.0 (from init params)\n",
      "Preparing 9690 prompts for HF batching...\n",
      "Sorting completed in 0.60 seconds.\n",
      "Processing in 303 batches of size 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 303/303 [02:02<00:00,  2.47it/s]\n",
      "Processing models: 100%|██████████| 1/1 [02:48<00:00, 168.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing all batches for meta-llama/Llama-3.1-8B.\n",
      "Releasing HF model and tokenizer for meta-llama/Llama-3.1-8B from cuda...\n",
      "HF resources released.\n",
      "--- Finished processing Model: meta-llama/Llama-3.1-8B with params: {'batch_size': 32} ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 1/1 [02:48<00:00, 168.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to data/logprobs.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[\n",
    "            \"meta-llama/Llama-3.1-8B\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 32},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"combinations\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for meta-llama/Llama-3.2-3B lacks a pad token. Using EOS token (<|end_of_text|>) for padding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e426b8894001442695b93a48ab2ddcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:06<00:00,  1.93it/s]\n",
      "Processing models:  50%|█████     | 1/2 [00:28<00:28, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for meta-llama/Llama-3.2-1B lacks a pad token. Using EOS token (<|end_of_text|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:02<00:00,  4.90it/s]\n",
      "Processing models: 100%|██████████| 2/2 [00:40<00:00, 20.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[\n",
    "            \"meta-llama/Llama-3.2-3B\",\n",
    "            \"meta-llama/Llama-3.2-1B\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 64},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"combinations\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using combinations mapping: running each model with each parameter set and each model_kwargs set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 64, 'device': 'mps'} ---\n",
      "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
      "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 64, 'device': 'mps', 'verbose': True}\n",
      "Initializing HFInterface for openai-community/gpt2\n",
      "Using device: mps\n",
      "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
      "Attempting to load model with identifier: 'openai-community/gpt2'\n",
      "Model openai-community/gpt2 loaded successfully on mps.\n",
      "Generated 9 prompts for model openai-community/gpt2 with params {'batch_size': 64, 'device': 'mps'}. Processing via HFInterface...\n",
      "Using batch size: 64, temperature: 1.0 (from init params)\n",
      "Preparing 9 prompts for HF batching...\n",
      "Sorting completed in 0.00 seconds.\n",
      "Processing in 1 batches of size 64...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing all batches for openai-community/gpt2.\n",
      "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n",
      "HF resources released.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing models: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 64, 'device': 'mps'} ---\n",
      "Saving results to data/test_new_logprobs.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/test_data_new_schema.json\",\n",
    "        models=[\n",
    "            \"openai-community/gpt2\",\n",
    "            # \"openai-community/gpt2-medium\",\n",
    "            # \"openai-community/gpt2-large\",\n",
    "            # \"openai-community/gpt2-xl\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 64, \"device\": \"mps\"},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"combinations\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/test_new_logprobs_refactored.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 34.44it/s]\n",
      "Processing models:  25%|██▌       | 1/4 [00:03<00:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-medium lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 14.80it/s]\n",
      "Processing models:  50%|█████     | 2/4 [00:10<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-large lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
      "Processing models:  75%|███████▌  | 3/4 [00:25<00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-xl lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:03<00:00,  3.56it/s]\n",
      "Processing models: 100%|██████████| 4/4 [00:46<00:00, 11.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[    \n",
    "            \"Qwen/Qwen2.5-32B\",\n",
    "            \"Qwen/Qwen2.5-14B\",\n",
    "            \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "            \"Qwen/Qwen2.5-7B\",\n",
    "            \"Qwen/Qwen2.5-3B\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 8},\n",
    "            {\"batch_size\": 16},\n",
    "            {\"batch_size\": 16},\n",
    "            {\"batch_size\": 32},\n",
    "            {\"batch_size\": 64},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"one_to_one\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 34.44it/s]\n",
      "Processing models:  25%|██▌       | 1/4 [00:03<00:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-medium lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 14.80it/s]\n",
      "Processing models:  50%|█████     | 2/4 [00:10<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-large lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
      "Processing models:  75%|███████▌  | 3/4 [00:25<00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-xl lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:03<00:00,  3.56it/s]\n",
      "Processing models: 100%|██████████| 4/4 [00:46<00:00, 11.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[\n",
    "            \"tiiuae/Falcon3-10B-Base\",\n",
    "            \"tiiuae/Falcon3-7B-Base\",\n",
    "            \"tiiuae/Falcon3-3B-Base\",\n",
    "            \"tiiuae/Falcon3-1B-Base\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 16},\n",
    "            {\"batch_size\": 16},\n",
    "            {\"batch_size\": 32},\n",
    "            {\"batch_size\": 64},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"one_to_one\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 34.44it/s]\n",
      "Processing models:  25%|██▌       | 1/4 [00:03<00:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-medium lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 14.80it/s]\n",
      "Processing models:  50%|█████     | 2/4 [00:10<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-large lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
      "Processing models:  75%|███████▌  | 3/4 [00:25<00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-xl lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:03<00:00,  3.56it/s]\n",
      "Processing models: 100%|██████████| 4/4 [00:46<00:00, 11.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[\n",
    "            \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "            \"mistralai/Mistral-Small-24B-Base-2501\",\n",
    "            \"mistralai/Mistral-Nemo-Base-2407\",\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 8},\n",
    "            {\"batch_size\": 16},\n",
    "            {\"batch_size\": 32},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"one_to_one\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 34.44it/s]\n",
      "Processing models:  25%|██▌       | 1/4 [00:03<00:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-medium lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:00<00:00, 14.80it/s]\n",
      "Processing models:  50%|█████     | 2/4 [00:10<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-large lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
      "Processing models:  75%|███████▌  | 3/4 [00:25<00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for openai-community/gpt2-xl lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [00:03<00:00,  3.56it/s]\n",
      "Processing models: 100%|██████████| 4/4 [00:46<00:00, 11.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.logprobs import collect_logprobs\n",
    "\n",
    "try:\n",
    "    collect_logprobs(\n",
    "        \"data/data.json\",\n",
    "        models=[    \n",
    "            \"microsoft/phi-4\"\n",
    "        ],\n",
    "        model_params=[\n",
    "            {\"batch_size\": 16},\n",
    "        ],\n",
    "        model_provider=\"hf\",\n",
    "        param_mapping_strategy=\"one_to_one\",\n",
    "        save_results=True,\n",
    "        save_path=\"data/logprobs.csv\",\n",
    "        verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer for meta-llama/Llama-3.1-70B lacks a pad token. Using EOS token (<|end_of_text|>) for padding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aca62155920444dbd2066985a7f946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from src.logprobs import collect_logprobs\n",
    "\n",
    "# try:\n",
    "#     collect_logprobs(\n",
    "#         \"data/data.json\",\n",
    "#         models=[\n",
    "#             \"meta-llama/Llama-3.1-70B\",\n",
    "#         ],\n",
    "#         model_params=[\n",
    "#             {\"batch_size\": 8},\n",
    "#         ],\n",
    "#         model_provider=\"hf\",\n",
    "#         param_mapping_strategy=\"combinations\",\n",
    "#         save_results=True,\n",
    "#         save_path=\"data/logprobs.csv\",\n",
    "#         verbose=True,\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
