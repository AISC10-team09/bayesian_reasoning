{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# GPT-2 Temperature Analysis\n",
        "This notebook collects logprobs for GPT-2 across different temperature settings to analyze the effect of temperature on Bayesian reasoning consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from src.logprobs import collect_logprobs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature settings:\n",
            "  1. 0.010\n",
            "  2. 0.032\n",
            "  3. 0.100\n",
            "  4. 0.316\n",
            "  5. 1.000\n",
            "  6. 2.000\n"
          ]
        }
      ],
      "source": [
        "# Define temperature settings: 10^(-2, -1.5, -1, -0.5, 0), 2\n",
        "temperatures = [\n",
        "    10**(-2),    # 0.01\n",
        "    10**(-1.5),  # ~0.032\n",
        "    10**(-1),    # 0.1\n",
        "    10**(-0.5),  # ~0.316\n",
        "    10**(0),     # 1.0\n",
        "    2\n",
        "]\n",
        "\n",
        "print(\"Temperature settings:\")\n",
        "for i, temp in enumerate(temperatures):\n",
        "    print(f\"  {i+1}. {temp:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running GPT-2 with 6 different temperature settings...\n",
            "Model parameters:\n",
            "  1. {'batch_size': 32, 'temperature': 0.01}\n",
            "  2. {'batch_size': 32, 'temperature': 0.03162277660168379}\n",
            "  3. {'batch_size': 32, 'temperature': 0.1}\n",
            "  4. {'batch_size': 32, 'temperature': 0.31622776601683794}\n",
            "  5. {'batch_size': 32, 'temperature': 1}\n",
            "  6. {'batch_size': 32, 'temperature': 2}\n"
          ]
        }
      ],
      "source": [
        "# Prepare model parameters for each temperature\n",
        "model_params = [{\"batch_size\": 32, \"temperature\": temp} for temp in temperatures]\n",
        "\n",
        "print(f\"Running GPT-2 with {len(model_params)} different temperature settings...\")\n",
        "print(\"Model parameters:\")\n",
        "for i, params in enumerate(model_params):\n",
        "    print(f\"  {i+1}. {params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using combinations mapping: running each model with each parameter set and each model_kwargs set.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 0.01} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 0.01, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 0.01}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 0.01 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.55 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [07:56<00:00,  1.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n",
            "HF resources released.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:  17%|█▋        | 1/6 [07:59<39:57, 479.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 0.01} ---\n",
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 0.03162277660168379} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 0.03162277660168379, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 0.03162277660168379}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 0.03162277660168379 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.60 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [23:46<00:00,  4.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:  33%|███▎      | 2/6 [31:50<1:09:16, 1039.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF resources released.\n",
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 0.03162277660168379} ---\n",
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 0.1} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 0.1, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 0.1}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 0.1 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.67 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [19:38<00:00,  3.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:  50%|█████     | 3/6 [51:35<55:17, 1105.88s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF resources released.\n",
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 0.1} ---\n",
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 0.31622776601683794} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 0.31622776601683794, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 0.31622776601683794}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 0.31622776601683794 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.70 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [14:22<00:00,  2.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:  67%|██████▋   | 4/6 [1:06:01<33:42, 1011.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF resources released.\n",
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 0.31622776601683794} ---\n",
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 1} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 1, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 1}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 1 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.55 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [12:40<00:00,  2.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:  83%|████████▎ | 5/6 [1:18:45<15:21, 921.96s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF resources released.\n",
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 1} ---\n",
            "\n",
            "--- Processing Model: openai-community/gpt2 with provider: hf and params: {'batch_size': 32, 'temperature': 2} ---\n",
            "Using HFInterface for openai-community/gpt2 with model_kwargs: {}\n",
            "Initializing LLMInterface for model: openai-community/gpt2 with params: {'model_kwargs': {}, 'batch_size': 32, 'temperature': 2, 'verbose': True}\n",
            "Initializing HFInterface for openai-community/gpt2\n",
            "Using device: mps\n",
            "Warning: Tokenizer for openai-community/gpt2 lacks a pad token. Using EOS token (<|endoftext|>) for padding.\n",
            "Attempting to load model with identifier: 'openai-community/gpt2'\n",
            "Model openai-community/gpt2 loaded successfully on mps.\n",
            "Generated 9690 prompts for model openai-community/gpt2 with params {'batch_size': 32, 'temperature': 2}. Processing via HFInterface...\n",
            "Using batch size: 32, temperature: 2 (from init params)\n",
            "Preparing 9690 prompts for HF batching...\n",
            "Sorting completed in 0.57 seconds.\n",
            "Processing in 303 batches of size 32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 303/303 [13:39<00:00,  2.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished processing all batches for openai-community/gpt2.\n",
            "Releasing HF model and tokenizer for openai-community/gpt2 from mps...\n",
            "HF resources released.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models: 100%|██████████| 6/6 [1:32:27<00:00, 924.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Finished processing Model: openai-community/gpt2 with params: {'batch_size': 32, 'temperature': 2} ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving results to data/temperature_logprobs.csv...\n",
            "\n",
            "Completed successfully! Results saved to data/temperature_logprobs.csv\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    collect_logprobs(\n",
        "        \"data/data.json\",\n",
        "        models=[\n",
        "            \"openai-community/gpt2\",\n",
        "        ],\n",
        "        model_params=model_params,\n",
        "        model_provider=\"hf\",\n",
        "        param_mapping_strategy=\"combinations\",\n",
        "        save_results=True,\n",
        "        save_path=\"data/temperature_logprobs.csv\",\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(\"\\nCompleted successfully! Results saved to data/temperature_logprobs.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-bayes",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
